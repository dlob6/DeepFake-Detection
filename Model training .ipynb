{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import fastai\n",
    "from fastai.text.models.transformer import MultiHeadAttention\n",
    "from fastai.vision import *\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "from skimage import io, transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements\n",
    "\n",
    "Data format:\n",
    "* Faces and STFTs must have been extracted with the notebook Extract faces and sound.ipynb.\n",
    "* In each video folder ('train_sample_videos', 'dfdc_train_videos_X', ...'), there must be a folder named 'face_frames' containing folders of faces and stfts for each video.\n",
    "\n",
    "Libraries:\n",
    "* pytorch\n",
    "* fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_ROOT_DIR = \"/media/dlo/New Volume/DeepFake/\"\n",
    "AUDIO_EXTENSION = \".jpg\"\n",
    "LABEL_FILE = \"metadata.json\"\n",
    "root_dirs = [\"dfdc_train_part_3/\"]\n",
    "\n",
    "# train on all set\n",
    "#root_dirs = glob(f\"{PATH_ROOT_DIR}/*/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################\n",
    "# Data preparation (indexing of extracted faces & stft per video) #\n",
    "# Custom pytorch dataset creation                                 #\n",
    "###################################################################\n",
    "\n",
    "# Data preparation\n",
    "class DeepFakeDF():\n",
    "    \"\"\" Indexes the faces & STFTs into a dataframe, to be used\n",
    "    by the pytorch dataset. processing is a bit long\"\"\"\n",
    "    def __init__(self, data_dirs, test = False):\n",
    "        self.data_dirs = [f\"{PATH_ROOT_DIR}{d}\" for d in data_dirs]\n",
    "        self.frame_dirs = [f\"{d}face_frames/\" for d in self.data_dirs]\n",
    "        print(self.frame_dirs)\n",
    "        audio = []\n",
    "        video = []\n",
    "        for frame_dir in self.frame_dirs:\n",
    "            audio.extend(glob.glob(f\"{frame_dir}*/audio*\"))\n",
    "            video.extend(glob.glob(f\"{frame_dir}*/webcam*\"))\n",
    "            \n",
    "        if not test:\n",
    "            self.labels = self._get_labels()\n",
    "        else:\n",
    "            self.labels = {}\n",
    "            \n",
    "        self.video = video\n",
    "        self.df = self._prep_df_audio(audio)\n",
    "        self.video_dicts = self._prep_video_dicts(video)\n",
    "        self._merge_audio_video()\n",
    "        \n",
    "    def get_df(self):\n",
    "        return self.df\n",
    "        \n",
    "    def _get_labels(self):\n",
    "        labels = {}\n",
    "        for d in self.data_dirs:\n",
    "            with open(f\"{d}{LABEL_FILE}\", \"r\") as f:\n",
    "                labels.update({f\"{k.split('.mp4')[0]}\": v['label'] \n",
    "                             for k, v in json.load(f).items()})\n",
    "                \n",
    "        return labels\n",
    "    \n",
    "    def _merge_audio_video(self):\n",
    "        \"\"\"\n",
    "        Audio has 1 sample per frame in any case. But the face extractor may have \n",
    "        missed some faces. This function attempts to provide a face for each audio sample.\n",
    "        \"\"\"\n",
    "        self.df['dir'] = self.df['audio'].str.split(\"/\").str[-4]\n",
    "        \n",
    "        # Actor 0:\n",
    "        # Flagging frames for which actor 0 was detected\n",
    "        self.df['actor_0'] = [self.video_dicts[0].get(tuple(o),np.nan) \n",
    "                              for o in self.df[['video_name', 'sample']].values.tolist()]\n",
    "        \n",
    "        # Creating path variables for frames in which actor 0 was detected\n",
    "        act0 = self.df.loc[~self.df['actor_0'].isna()].copy()\n",
    "        act0['actor_0'] = (PATH_ROOT_DIR + act0['dir'] + \"/face_frames/\" + act0['video_name'] \n",
    "                           + \"/\" + \"webcam_\" + act0['sample'].astype(str) + \"_0\" \n",
    "                           + \".jpg\")\n",
    "        self.df.loc[~self.df['actor_0'].isna(), 'actor_0'] = act0\n",
    "        \n",
    "        # Actor 1:\n",
    "        # Flagging frames for which actor 1 was detected\n",
    "        self.df['actor_1'] = [self.video_dicts[1].get(tuple(o),np.nan) \n",
    "                              for o in self.df[['video_name', 'sample']].values.tolist()]\n",
    "        # Creating path variables for frames in which actor 1 was detected\n",
    "        act1 = self.df.loc[~self.df['actor_1'].isna()].copy()\n",
    "        act1['actor_1'] = (PATH_ROOT_DIR + act1['dir'] + \"/face_frames/\" + act1['video_name'] \n",
    "                           + \"/\" + \"webcam_\" + act1['sample'].astype(str) + \"_1\" \n",
    "                           + \".jpg\")\n",
    "        self.df.loc[~self.df['actor_1'].isna(), 'actor_1'] = act1\n",
    "        \n",
    "        # Filling NaNs. Forward fill per video name, so that missing faces are replaced\n",
    "        # by the previous detected face.\n",
    "        for vid in self.df['video_name'].unique():\n",
    "            cond = (self.df['video_name'] == vid)\n",
    "            \n",
    "            self.df.loc[cond,'actor_0'] = (self.df.loc[cond,'actor_0']\n",
    "                                           .fillna(method = 'ffill')\n",
    "                                           .fillna(method = 'bfill'))\n",
    "            \n",
    "            self.df.loc[cond,'actor_1'] = (self.df.loc[cond,'actor_1']\n",
    "                                           .fillna(method = 'ffill')\n",
    "                                           .fillna(method = 'bfill'))\n",
    "        \n",
    "        # As not all videos have two actors, for now, simply copying the 1st actor into the 2nd\n",
    "        # actor field when there is only 1 actor.\n",
    "        self.df.loc[self.df['actor_1'].isna(), 'actor_1'] = self.df['actor_0']\n",
    "        \n",
    "        for col in ['audio', 'actor_0', 'actor_1']:\n",
    "            self.df[col] = self.df[col].str.replace(PATH_ROOT_DIR,\"\")\n",
    "        \n",
    "    \n",
    "    def _prep_df_audio(self, audio):\n",
    "        \"\"\"Returns a dataframe indexed on frames of videos.\n",
    "        Contains the path to each .jpg of STFTs of video frames\"\"\"\n",
    "        df = pd.DataFrame(audio, columns = ['audio'])\n",
    "        df['video_name'] = df['audio'].str.split(\"/\").str[-2]\n",
    "        df['sample'] = df['audio'].str.split(\"/\").str[-1].str.split(\".\").str[0].str.split(\"_\").str[-1].astype(int)\n",
    "        df['label'] = df['video_name'].apply(lambda x: self.labels.get(x,\"\"))\n",
    "        df.sort_values(by=['video_name','sample'], inplace = True)\n",
    "        df['actor_0'] = \"\"\n",
    "        df['actor_1'] = \"\"\n",
    "        return df\n",
    "    \n",
    "    def _prep_video_dicts(self, video):\n",
    "        \"\"\"Returns dicts, one that tell if a face was detected in frames of videos,\n",
    "        and one that tells if a second face was detected in frames of videos.\"\"\"\n",
    "        video_name, frame_name = zip(*[o.split(\"/\")[-2:] for o in video])\n",
    "        samples, actors = zip(*[o.replace(\".jpg\",\"\").split(\"_\")[-2:] for o in frame_name])\n",
    "        samples = [int(o) for o in samples]\n",
    "        actors = [int(o) for o in actors] \n",
    "        #samples = [int(o) for o in actors]\n",
    "        #actors = [0 for o in samples]\n",
    "        actor_0_present = {(v, s) : a for v, s, a in zip(video_name, samples, actors) if a == 0}\n",
    "        actor_1_present = {(v, s) : a for v, s, a in zip(video_name, samples, actors) if a == 1}\n",
    "        return actor_0_present, actor_1_present\n",
    "\n",
    "    \n",
    "# Custom pytorch dataset\n",
    "class DeepFakeJPGDataset(Dataset):\n",
    "    \"\"\"DeepFakeJPGDataset. Opens .jpgs of either faces or STFTs for each frame of video.\n",
    "    Returns tensors of a concatenatetion of the video's frames.\"\"\"\n",
    "\n",
    "    def __init__(self, df, col_name, transform = None, downsample_factor = 1):\n",
    "        \"\"\"df[col_name] has to contain paths to .jpg files,\n",
    "        either of faces or of STFTs.\n",
    "        transform: resize images - all cropped faces don't have the same\n",
    "        shape, they won't fit together in a batch. Need to resize them,\n",
    "        use transforms.Resize((150,100)) for example. \n",
    "        downsample_factor: use > 1 to not use all the frames of a video\n",
    "        \"\"\"\n",
    "        self.x = df[col_name]\n",
    "        self.y = df['label'].astype('category').cat.codes.astype(int)\n",
    "        self.transform = transform\n",
    "        self.downsample_factor = int(downsample_factor)\n",
    "        self.col_name = col_name\n",
    "\n",
    "    def __len__(self): return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        list_image_paths = self.x.iloc[idx]\n",
    "        # Opening one every 'downsample_factor' image\n",
    "        images = [PIL.Image.open(PATH_ROOT_DIR + im) \n",
    "                  for im in list_image_paths[::self.downsample_factor]]\n",
    "        target = self.y.iloc[idx]\n",
    "        \n",
    "        # Resizing\n",
    "        if self.transform:\n",
    "            images = [self.transform(im) for im in images]\n",
    "            \n",
    "        # Normalizing the .jpgs to [-0.5, 0.5] both for \n",
    "        # faces and sound STFTs.\n",
    "        # TODO: Normalize faces with imagenet stats, as the \n",
    "        # model taking faces as input is pretrained on imagenet\n",
    "        images = [(np.array(im) / 255.0) - 0.5 for im in images]\n",
    "        \n",
    "        # Adding channel dimension to STFT images (grayscale)\n",
    "        if len(images[0].shape) == 2: \n",
    "            images = [im[...,None] for im in images]\n",
    "            \n",
    "        # (n_frames, channels, height, width)    \n",
    "        return torch.Tensor(images).permute(0,3,1,2)\n",
    "    \n",
    "class DeepFakeDetectionDataset(Dataset):\n",
    "    \"\"\"DeepFakeDetectionDataset. Merges faces & STFT datasets.\"\"\"\n",
    "    def __init__(self, x1, x2, y):\n",
    "        self.x1,self.x2,self.y = x1,x2,y\n",
    "    def __len__(self): \n",
    "        return len(self.y)\n",
    "    def __getitem__(self, i): \n",
    "        return (self.x1[i], self.x2[i]), self.y[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/media/dlo/New Volume/DeepFake/dfdc_train_part_3/face_frames/']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         gmutjqhliq\n",
      "1         gmutjqhliq\n",
      "2         gmutjqhliq\n",
      "3         gmutjqhliq\n",
      "4         gmutjqhliq\n",
      "5         gmutjqhliq\n",
      "6         gmutjqhliq\n",
      "7         gmutjqhliq\n",
      "8         gmutjqhliq\n",
      "9         gmutjqhliq\n",
      "10        gmutjqhliq\n",
      "11        gmutjqhliq\n",
      "12        gmutjqhliq\n",
      "13        gmutjqhliq\n",
      "14        gmutjqhliq\n",
      "15        gmutjqhliq\n",
      "16        gmutjqhliq\n",
      "17        gmutjqhliq\n",
      "18        gmutjqhliq\n",
      "19        gmutjqhliq\n",
      "20        gmutjqhliq\n",
      "21        gmutjqhliq\n",
      "22        gmutjqhliq\n",
      "23        gmutjqhliq\n",
      "24        gmutjqhliq\n",
      "25        gmutjqhliq\n",
      "26        gmutjqhliq\n",
      "27        gmutjqhliq\n",
      "28        gmutjqhliq\n",
      "29        gmutjqhliq\n",
      "             ...    \n",
      "436470    zzrppdukls\n",
      "436471    zzrppdukls\n",
      "436472    zzrppdukls\n",
      "436473    zzrppdukls\n",
      "436474    zzrppdukls\n",
      "436475    zzrppdukls\n",
      "436476    zzrppdukls\n",
      "436477    zzrppdukls\n",
      "436478    zzrppdukls\n",
      "436479    zzrppdukls\n",
      "436480    zzrppdukls\n",
      "436481    zzrppdukls\n",
      "436482    zzrppdukls\n",
      "436483    zzrppdukls\n",
      "436484    zzrppdukls\n",
      "436485    zzrppdukls\n",
      "436486    zzrppdukls\n",
      "436487    zzrppdukls\n",
      "436488    zzrppdukls\n",
      "436489    zzrppdukls\n",
      "436490    zzrppdukls\n",
      "436491    zzrppdukls\n",
      "436492    zzrppdukls\n",
      "436493    zzrppdukls\n",
      "436494    zzrppdukls\n",
      "436495    zzrppdukls\n",
      "436496    zzrppdukls\n",
      "436497    zzrppdukls\n",
      "436498    zzrppdukls\n",
      "436499    zzrppdukls\n",
      "Name: video_name, Length: 436500, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df = DeepFakeDF(root_dirs).get_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.get_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>audio</th>\n",
       "      <th>video_name</th>\n",
       "      <th>sample</th>\n",
       "      <th>label</th>\n",
       "      <th>actor_0</th>\n",
       "      <th>actor_1</th>\n",
       "      <th>dir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>dfdc_train_part_3/face_frames/aafezqchru/audio...</td>\n",
       "      <td>aafezqchru</td>\n",
       "      <td>0</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>dfdc_train_part_3/face_frames/aafezqchru/webca...</td>\n",
       "      <td>dfdc_train_part_3/face_frames/aafezqchru/webca...</td>\n",
       "      <td>dfdc_train_part_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>dfdc_train_part_3/face_frames/aafezqchru/audio...</td>\n",
       "      <td>aafezqchru</td>\n",
       "      <td>1</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>dfdc_train_part_3/face_frames/aafezqchru/webca...</td>\n",
       "      <td>dfdc_train_part_3/face_frames/aafezqchru/webca...</td>\n",
       "      <td>dfdc_train_part_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>dfdc_train_part_3/face_frames/aafezqchru/audio...</td>\n",
       "      <td>aafezqchru</td>\n",
       "      <td>2</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>dfdc_train_part_3/face_frames/aafezqchru/webca...</td>\n",
       "      <td>dfdc_train_part_3/face_frames/aafezqchru/webca...</td>\n",
       "      <td>dfdc_train_part_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>dfdc_train_part_3/face_frames/aafezqchru/audio...</td>\n",
       "      <td>aafezqchru</td>\n",
       "      <td>3</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>dfdc_train_part_3/face_frames/aafezqchru/webca...</td>\n",
       "      <td>dfdc_train_part_3/face_frames/aafezqchru/webca...</td>\n",
       "      <td>dfdc_train_part_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>dfdc_train_part_3/face_frames/aafezqchru/audio...</td>\n",
       "      <td>aafezqchru</td>\n",
       "      <td>4</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>dfdc_train_part_3/face_frames/aafezqchru/webca...</td>\n",
       "      <td>dfdc_train_part_3/face_frames/aafezqchru/webca...</td>\n",
       "      <td>dfdc_train_part_3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 audio  video_name  sample  \\\n",
       "600  dfdc_train_part_3/face_frames/aafezqchru/audio...  aafezqchru       0   \n",
       "601  dfdc_train_part_3/face_frames/aafezqchru/audio...  aafezqchru       1   \n",
       "799  dfdc_train_part_3/face_frames/aafezqchru/audio...  aafezqchru       2   \n",
       "887  dfdc_train_part_3/face_frames/aafezqchru/audio...  aafezqchru       3   \n",
       "898  dfdc_train_part_3/face_frames/aafezqchru/audio...  aafezqchru       4   \n",
       "\n",
       "    label                                            actor_0  \\\n",
       "600  FAKE  dfdc_train_part_3/face_frames/aafezqchru/webca...   \n",
       "601  FAKE  dfdc_train_part_3/face_frames/aafezqchru/webca...   \n",
       "799  FAKE  dfdc_train_part_3/face_frames/aafezqchru/webca...   \n",
       "887  FAKE  dfdc_train_part_3/face_frames/aafezqchru/webca...   \n",
       "898  FAKE  dfdc_train_part_3/face_frames/aafezqchru/webca...   \n",
       "\n",
       "                                               actor_1                dir  \n",
       "600  dfdc_train_part_3/face_frames/aafezqchru/webca...  dfdc_train_part_3  \n",
       "601  dfdc_train_part_3/face_frames/aafezqchru/webca...  dfdc_train_part_3  \n",
       "799  dfdc_train_part_3/face_frames/aafezqchru/webca...  dfdc_train_part_3  \n",
       "887  dfdc_train_part_3/face_frames/aafezqchru/webca...  dfdc_train_part_3  \n",
       "898  dfdc_train_part_3/face_frames/aafezqchru/webca...  dfdc_train_part_3  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset format - indexed on frames, simple use case where predictions \n",
    "# are made independently on each frame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/media/dlo/New Volume/DeepFake/test_videos/face_frames/']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         aassnaulhq\n",
      "1         aassnaulhq\n",
      "2         aassnaulhq\n",
      "3         aassnaulhq\n",
      "4         aassnaulhq\n",
      "5         aassnaulhq\n",
      "6         aassnaulhq\n",
      "7         aassnaulhq\n",
      "8         aassnaulhq\n",
      "9         aassnaulhq\n",
      "10        aassnaulhq\n",
      "11        aassnaulhq\n",
      "12        aassnaulhq\n",
      "13        aassnaulhq\n",
      "14        aassnaulhq\n",
      "15        aassnaulhq\n",
      "16        aassnaulhq\n",
      "17        aassnaulhq\n",
      "18        aassnaulhq\n",
      "19        aassnaulhq\n",
      "20        aassnaulhq\n",
      "21        aassnaulhq\n",
      "22        aassnaulhq\n",
      "23        aassnaulhq\n",
      "24        aassnaulhq\n",
      "25        aassnaulhq\n",
      "26        aassnaulhq\n",
      "27        aassnaulhq\n",
      "28        aassnaulhq\n",
      "29        aassnaulhq\n",
      "             ...    \n",
      "119970    zzmgnglanj\n",
      "119971    zzmgnglanj\n",
      "119972    zzmgnglanj\n",
      "119973    zzmgnglanj\n",
      "119974    zzmgnglanj\n",
      "119975    zzmgnglanj\n",
      "119976    zzmgnglanj\n",
      "119977    zzmgnglanj\n",
      "119978    zzmgnglanj\n",
      "119979    zzmgnglanj\n",
      "119980    zzmgnglanj\n",
      "119981    zzmgnglanj\n",
      "119982    zzmgnglanj\n",
      "119983    zzmgnglanj\n",
      "119984    zzmgnglanj\n",
      "119985    zzmgnglanj\n",
      "119986    zzmgnglanj\n",
      "119987    zzmgnglanj\n",
      "119988    zzmgnglanj\n",
      "119989    zzmgnglanj\n",
      "119990    zzmgnglanj\n",
      "119991    zzmgnglanj\n",
      "119992    zzmgnglanj\n",
      "119993    zzmgnglanj\n",
      "119994    zzmgnglanj\n",
      "119995    zzmgnglanj\n",
      "119996    zzmgnglanj\n",
      "119997    zzmgnglanj\n",
      "119998    zzmgnglanj\n",
      "119999    zzmgnglanj\n",
      "Name: video_name, Length: 120000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Dataset format - indexed on videos, use case where all the frames of a \n",
    "# video are used to make a TRUE/FAKE prediction.\n",
    "# Each video is assigned a list of all its extracted faces and all stfts.\n",
    "gb = df.groupby('video_name')\n",
    "audio = gb['audio'].apply(list)\n",
    "video = gb['actor_0'].apply(list)\n",
    "label = gb['label'].nth(0)\n",
    "df = pd.concat([audio,video,label],axis=1)\n",
    "df.reset_index(inplace = True)\n",
    "df.head()\n",
    "\n",
    "\"\"\"test_df = DeepFakeDF([\"test_videos/\"], test = True).get_df()\n",
    "\n",
    "gb = test_df.groupby('video_name')\n",
    "audio = gb['audio'].apply(list)\n",
    "video = gb['actor_0'].apply(list)\n",
    "label = gb['label'].nth(0)\n",
    "test_df = pd.concat([audio,video,label],axis=1)\n",
    "test_df.reset_index(inplace = True)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train/val split indices\n",
    "val_perc = 0.2\n",
    "n_val = int(val_perc*len(df))\n",
    "shuffled_idx = np.random.permutation(df.index.tolist())\n",
    "\n",
    "val_idx = shuffled_idx[:n_val]\n",
    "train_idx = shuffled_idx[n_val:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsample_factor = 10 # using one every 10 frames only\n",
    "n_frames = 300 // downsample_factor\n",
    "\n",
    "# train/val torch datasets & dataloaders\n",
    "tr_images = DeepFakeJPGDataset(df.iloc[train_idx].reset_index(drop = True), \n",
    "                               'actor_0', transforms.Resize((150,100)), downsample_factor)\n",
    "tr_sound = DeepFakeJPGDataset(df.iloc[train_idx].reset_index(drop = True),\n",
    "                               'audio', transforms.Resize((65,25)), downsample_factor)\n",
    "train_ds = DeepFakeDetectionDataset(tr_images, tr_sound, tr_images.y)\n",
    "train_dl = DataLoader(train_ds)\n",
    "\n",
    "val_images = DeepFakeJPGDataset(df.iloc[val_idx].reset_index(drop = True), \n",
    "                                'actor_0', transforms.Resize((150,100)), downsample_factor)\n",
    "val_sound = DeepFakeJPGDataset(df.iloc[val_idx].reset_index(drop = True), \n",
    "                               'audio', transforms.Resize((65,25)), downsample_factor)\n",
    "valid_ds = DeepFakeDetectionDataset(val_images, val_sound, val_images.y)\n",
    "valid_dl = DataLoader(valid_ds)\n",
    "\n",
    "\"\"\"te_images = DeepFakeJPGDataset(test_df.reset_index(drop = True), \n",
    "                               'actor_0', transforms.Resize((150,100)), downsample_factor)\n",
    "te_sound = DeepFakeJPGDataset(test_df.reset_index(drop = True),\n",
    "                               'audio', transforms.Resize((65,25)), downsample_factor)\n",
    "test_ds = DeepFakeDetectionDataset(te_images, te_sound, te_images.y)\n",
    "test_dl = DataLoader(test_ds)\"\"\"\n",
    "\n",
    "# fastai databunch\n",
    "db = DataBunch(train_dl,valid_dl)#,test_dl=test_dl)\n",
    "db.batch_size = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "# Models #\n",
    "##########\n",
    "\n",
    "# Frame embeddings\n",
    "def fully_connected(layers, dropout, bn = True):\n",
    "    \"\"\"Returns a series of [BatchNorm1d, Dropout, Linear]*len(layers)\n",
    "    The size of the linear layers is given by 'layers'. \"\"\"\n",
    "    model_layers = [] \n",
    "    activations = [nn.ReLU(inplace=True)] * (len(layers)-1)\n",
    "    for n_in, n_out, p, actn in zip(layers[:-1], layers[1:], dropout, activations):\n",
    "        model_layers += bn_drop_lin(n_in, n_out, p = p, actn = actn, bn = bn)\n",
    "    return nn.Sequential(*model_layers)\n",
    "\n",
    "class VideoAnalyzer(nn.Module):\n",
    "    def __init__(self, frame_embedding_size, n_frames): \n",
    "        super().__init__()\n",
    "        self.self_attention = MultiHeadAttention(5,frame_embedding_size)\n",
    "        layers = [n_frames*frame_embedding_size, n_frames*frame_embedding_size // 2, 50]\n",
    "        self.linears = fully_connected(layers, [0.1]*len(layers))\n",
    "        self.classifier = nn.Linear(layers[-1],2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.self_attention(x)\n",
    "        x = x.contiguous().view(x.size(0), -1)\n",
    "        x = self.linears(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "# Global model\n",
    "class DeepFakeDetector(nn.Module):\n",
    "    \"\"\"DeepFakeDetectionModel. This model has four main parts:\n",
    "    -Face analyzer: convnet 2d pretrained on imagenet.\n",
    "    -STFT analyzer: convnet 2d.\n",
    "    -Face & STFT merger: fully connected network, takes the output of the two above networks\n",
    "        as input, and outputs a vector (small dimension) representation of the frame's video and\n",
    "        audio - frame embeddings.\n",
    "    -Video analyzer: once the three above network have processed all the frames of a video, \n",
    "        the concatenation of the frame embeddings is passed to a 4th network. This network sees\n",
    "        the entire video through its frame embeddings, and predicts the label TRUE/FAKE.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_face, model_stft, model_merge, model_video, n_frames): \n",
    "        super().__init__()\n",
    "        self.n_frames = n_frames\n",
    "        \n",
    "        # two conv heads\n",
    "        self.model_face = model_face\n",
    "        self.model_stft = model_stft\n",
    "        self.poolflat = fastai.layers.PoolFlatten()\n",
    "        \n",
    "        # frame embeddings\n",
    "        self.model_merge = model_merge\n",
    "        \n",
    "        # frame embeddings aggregator, and classifier\n",
    "        self.model_video = model_video\n",
    "\n",
    "    def forward(self, *x):\n",
    "        x_faces = x[0]\n",
    "        x_stfts = x[1]\n",
    "        \n",
    "        frame_embeddings = []\n",
    "        for frame in range(self.n_frames):\n",
    "            \n",
    "            x_face = self.model_face(x_faces[:,frame,:,:,:])\n",
    "            x_face = self.poolflat(x_face)\n",
    "            x_stft = self.model_stft(x_stfts[:,frame,:,:,:])\n",
    "            x = torch.cat([x_face, x_stft], dim=1)\n",
    "            x = self.model_merge(x)\n",
    "            frame_embeddings.append(x[:,None,:])\n",
    "        \n",
    "        x = torch.cat(frame_embeddings, dim = 1)\n",
    "        x = self.model_video(x)\n",
    "        return F.log_softmax(x, dim = -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Submodels\n",
    "model_faces = create_body(fastai.vision.models.resnet18)\n",
    "\n",
    "model_stfts = simple_cnn(actns = [1,8,16,32,64], strides = [(2,1),(2,2),(2,2),(2,2)],\n",
    "                         bn = True)\n",
    "\n",
    "frame_embedding_size = 32\n",
    "merge_layers = [512 + 64, frame_embedding_size]\n",
    "model_frame = fully_connected(merge_layers, dropout = [0.1]*len(merge_layers))\n",
    "\n",
    "model_video = VideoAnalyzer(frame_embedding_size, n_frames)\n",
    "# Global model\n",
    "model = DeepFakeDetector(model_faces, model_stfts, model_frame, \n",
    "                             model_video, n_frames = n_frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dlo/anaconda3/envs/fastai_gpu/lib/python3.7/site-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type DeepFakeDetector. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/dlo/anaconda3/envs/fastai_gpu/lib/python3.7/site-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type VideoAnalyzer. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "learn = Learner(db, model, metrics = [accuracy]).load('20_ep_51e-4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x1,x2), y = next(iter(db.test_dl))\n",
    "x1 = x1.to('cuda')\n",
    "x2 = x2.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 512, 5, 4])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_faces(x1[:,0,:,:]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 64])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_stfts(x2[:,0,:,:]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 1:25:31 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.811163</td>\n",
       "      <td>0.781662</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>04:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.742356</td>\n",
       "      <td>0.698815</td>\n",
       "      <td>0.425000</td>\n",
       "      <td>04:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.628156</td>\n",
       "      <td>0.556962</td>\n",
       "      <td>0.712500</td>\n",
       "      <td>04:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.507033</td>\n",
       "      <td>0.525933</td>\n",
       "      <td>0.775000</td>\n",
       "      <td>04:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.428797</td>\n",
       "      <td>0.676499</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>04:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.424062</td>\n",
       "      <td>0.420685</td>\n",
       "      <td>0.787500</td>\n",
       "      <td>04:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.424210</td>\n",
       "      <td>0.418869</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>04:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.388338</td>\n",
       "      <td>0.347876</td>\n",
       "      <td>0.862500</td>\n",
       "      <td>04:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.359970</td>\n",
       "      <td>0.299131</td>\n",
       "      <td>0.912500</td>\n",
       "      <td>04:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.310886</td>\n",
       "      <td>0.464591</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>04:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.290028</td>\n",
       "      <td>0.392199</td>\n",
       "      <td>0.837500</td>\n",
       "      <td>04:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.260943</td>\n",
       "      <td>0.265375</td>\n",
       "      <td>0.887500</td>\n",
       "      <td>04:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.217971</td>\n",
       "      <td>0.298527</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>04:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.157591</td>\n",
       "      <td>0.244809</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>04:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.107387</td>\n",
       "      <td>0.320370</td>\n",
       "      <td>0.862500</td>\n",
       "      <td>04:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.072163</td>\n",
       "      <td>0.223150</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>04:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.050639</td>\n",
       "      <td>0.232323</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>04:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.035849</td>\n",
       "      <td>0.240605</td>\n",
       "      <td>0.912500</td>\n",
       "      <td>04:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.027176</td>\n",
       "      <td>0.246818</td>\n",
       "      <td>0.912500</td>\n",
       "      <td>04:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.022309</td>\n",
       "      <td>0.244412</td>\n",
       "      <td>0.912500</td>\n",
       "      <td>04:15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(20,5*1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 15:30 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.387173</td>\n",
       "      <td>0.484208</td>\n",
       "      <td>0.718213</td>\n",
       "      <td>15:30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(4,1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 13:43 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.603848</td>\n",
       "      <td>0.540501</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>01:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.405393</td>\n",
       "      <td>0.440084</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>01:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.325628</td>\n",
       "      <td>0.435164</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>01:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.304964</td>\n",
       "      <td>0.367607</td>\n",
       "      <td>0.862500</td>\n",
       "      <td>01:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.264422</td>\n",
       "      <td>0.386004</td>\n",
       "      <td>0.837500</td>\n",
       "      <td>01:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.207702</td>\n",
       "      <td>0.324583</td>\n",
       "      <td>0.862500</td>\n",
       "      <td>01:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.132341</td>\n",
       "      <td>0.315138</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>01:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.074944</td>\n",
       "      <td>0.323226</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>01:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.041357</td>\n",
       "      <td>0.328530</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>01:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.025301</td>\n",
       "      <td>0.329194</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>01:21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(10,5*1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For inference on the test set\n",
    "learn.save('20_ep_51e-4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
