{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import fastai\n",
    "from fastai.vision import *\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "from skimage import io, transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements\n",
    "\n",
    "Data format:\n",
    "* Faces and STFTs must have been extracted with the notebook Extract faces and sound.ipynb.\n",
    "* In each video folder ('train_sample_videos', 'dfdc_train_videos_X', ...'), there must be a folder named 'face_frames' containing folders of faces and stfts for each video.\n",
    "\n",
    "Libraries:\n",
    "* pytorch\n",
    "* fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_ROOT_DIR = \"/media/dlo/New Volume/DeepFake/\"\n",
    "AUDIO_EXTENSION = \".jpg\"\n",
    "LABEL_FILE = \"metadata.json\"\n",
    "root_dirs = [\"train_sample_videos/\"]\n",
    "\n",
    "# train on all set\n",
    "#root_dirs = glob(f\"{PATH_ROOT_DIR}/*/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################\n",
    "# Data preparation (indexing of extracted faces & stft per video) #\n",
    "# Custom pytorch dataset creation                                 #\n",
    "###################################################################\n",
    "\n",
    "# Data preparation\n",
    "class DeepFakeDF():\n",
    "    \"\"\" Indexes the faces & STFTs into a dataframe, to be used\n",
    "    by the pytorch dataset. processing is a bit long\"\"\"\n",
    "    def __init__(self, data_dirs, test = False):\n",
    "        self.data_dirs = [f\"{PATH_ROOT_DIR}{d}\" for d in data_dirs]\n",
    "        self.frame_dirs = [f\"{d}face_frames/\" for d in self.data_dirs]\n",
    "        \n",
    "        audio = []\n",
    "        video = []\n",
    "        for frame_dir in self.frame_dirs:\n",
    "            audio.extend(glob.glob(f\"{frame_dir}*/audio*\"))\n",
    "            video.extend(glob.glob(f\"{frame_dir}*/webcam*\"))\n",
    "            \n",
    "        if not test:\n",
    "            self.labels = self._get_labels()\n",
    "        else:\n",
    "            self.labels = {}\n",
    "            \n",
    "        self.video = video\n",
    "        self.df = self._prep_df_audio(audio)\n",
    "        self.video_dicts = self._prep_video_dicts(video)\n",
    "        self._merge_audio_video()\n",
    "        \n",
    "    def get_df(self):\n",
    "        return self.df\n",
    "        \n",
    "    def _get_labels(self):\n",
    "        labels = {}\n",
    "        for d in self.data_dirs:\n",
    "            with open(f\"{d}{LABEL_FILE}\", \"r\") as f:\n",
    "                labels.update({f\"{k.split('.mp4')[0]}\": v['label'] \n",
    "                             for k, v in json.load(f).items()})\n",
    "                \n",
    "        return labels\n",
    "    \n",
    "    def _merge_audio_video(self):\n",
    "        \"\"\"\n",
    "        Audio has 1 sample per frame in any case. But the face extractor may have \n",
    "        missed some faces. This function attempts to provide a face for each audio sample.\n",
    "        \"\"\"\n",
    "        self.df['dir'] = self.df['audio'].str.split(\"/\").str[-4]\n",
    "        \n",
    "        # Actor 0:\n",
    "        # Flagging frames for which actor 0 was detected\n",
    "        self.df['actor_0'] = [self.video_dicts[0].get(tuple(o),np.nan) \n",
    "                              for o in self.df[['video_name', 'sample']].values.tolist()]\n",
    "        \n",
    "        # Creating path variables for frames in which actor 0 was detected\n",
    "        act0 = self.df.loc[~self.df['actor_0'].isna()].copy()\n",
    "        act0['actor_0'] = (PATH_ROOT_DIR + act0['dir'] + \"/face_frames/\" + act0['video_name'] \n",
    "                           + \"/\" + \"webcam_\" + act0['sample'].astype(str) + \"_0\" \n",
    "                           + \".jpg\")\n",
    "        self.df.loc[~self.df['actor_0'].isna(), 'actor_0'] = act0\n",
    "        \n",
    "        # Actor 1:\n",
    "        # Flagging frames for which actor 1 was detected\n",
    "        self.df['actor_1'] = [self.video_dicts[1].get(tuple(o),np.nan) \n",
    "                              for o in self.df[['video_name', 'sample']].values.tolist()]\n",
    "        # Creating path variables for frames in which actor 1 was detected\n",
    "        act1 = self.df.loc[~self.df['actor_1'].isna()].copy()\n",
    "        act1['actor_1'] = (PATH_ROOT_DIR + act1['dir'] + \"/face_frames/\" + act1['video_name'] \n",
    "                           + \"/\" + \"webcam_\" + act1['sample'].astype(str) + \"_1\" \n",
    "                           + \".jpg\")\n",
    "        self.df.loc[~self.df['actor_1'].isna(), 'actor_1'] = act1\n",
    "        \n",
    "        # Filling NaNs. Forward fill per video name, so that missing faces are replaced\n",
    "        # by the previous detected face.\n",
    "        for vid in self.df['video_name'].unique():\n",
    "            cond = (self.df['video_name'] == vid)\n",
    "            \n",
    "            self.df.loc[cond,'actor_0'] = (self.df.loc[cond,'actor_0']\n",
    "                                           .fillna(method = 'ffill')\n",
    "                                           .fillna(method = 'bfill'))\n",
    "            \n",
    "            self.df.loc[cond,'actor_1'] = (self.df.loc[cond,'actor_1']\n",
    "                                           .fillna(method = 'ffill')\n",
    "                                           .fillna(method = 'bfill'))\n",
    "        \n",
    "        # As not all videos have two actors, for now, simply copying the 1st actor into the 2nd\n",
    "        # actor field when there is only 1 actor.\n",
    "        self.df.loc[self.df['actor_1'].isna(), 'actor_1'] = self.df['actor_0']\n",
    "        \n",
    "        for col in ['audio', 'actor_0', 'actor_1']:\n",
    "            self.df[col] = self.df[col].str.replace(PATH_ROOT_DIR,\"\")\n",
    "        \n",
    "    \n",
    "    def _prep_df_audio(self, audio):\n",
    "        \"\"\"Returns a dataframe indexed on frames of videos.\n",
    "        Contains the path to each .jpg of STFTs of video frames\"\"\"\n",
    "        df = pd.DataFrame(audio, columns = ['audio'])\n",
    "        df['video_name'] = df['audio'].str.split(\"/\").str[-2]\n",
    "        df['sample'] = df['audio'].str.split(\"/\").str[-1].str.split(\".\").str[0].str.split(\"_\").str[-1].astype(int)\n",
    "        df['label'] = df['video_name'].apply(lambda x: self.labels.get(x,\"\"))\n",
    "        df.sort_values(by=['video_name','sample'], inplace = True)\n",
    "        df['actor_0'] = \"\"\n",
    "        df['actor_1'] = \"\"\n",
    "        return df\n",
    "    \n",
    "    def _prep_video_dicts(self, video):\n",
    "        \"\"\"Returns dicts, one that tell if a face was detected in frames of videos,\n",
    "        and one that tells if a second face was detected in frames of videos.\"\"\"\n",
    "        video_name, frame_name = zip(*[o.split(\"/\")[-2:] for o in video])\n",
    "        samples, actors = zip(*[o.replace(\".jpg\",\"\").split(\"_\")[-2:] for o in frame_name])\n",
    "        samples = [int(o) for o in samples]\n",
    "        actors = [int(o) for o in actors]\n",
    "        actor_0_present = {(v, s) : a for v, s, a in zip(video_name, samples, actors) if a == 0}\n",
    "        actor_1_present = {(v, s) : a for v, s, a in zip(video_name, samples, actors) if a == 1}\n",
    "        return actor_0_present, actor_1_present\n",
    "\n",
    "    \n",
    "# Custom pytorch dataset\n",
    "class DeepFakeJPGDataset(Dataset):\n",
    "    \"\"\"DeepFakeJPGDataset. Opens .jpgs of either faces or STFTs for each frame of video.\n",
    "    Returns tensors of a concatenatetion of the video's frames.\"\"\"\n",
    "\n",
    "    def __init__(self, df, col_name, transform = None, downsample_factor = 1):\n",
    "        \"\"\"df[col_name] has to contain paths to .jpg files,\n",
    "        either of faces or of STFTs.\n",
    "        transform: resize images - all cropped faces don't have the same\n",
    "        shape, they won't fit together in a batch. Need to resize them,\n",
    "        use transforms.Resize((150,100)) for example. \n",
    "        downsample_factor: use > 1 to not use all the frames of a video\n",
    "        \"\"\"\n",
    "        self.x = df[col_name]\n",
    "        self.y = df['label'].astype('category').cat.codes.astype(int)\n",
    "        self.transform = transform\n",
    "        self.downsample_factor = int(downsample_factor)\n",
    "        self.col_name = col_name\n",
    "\n",
    "    def __len__(self): return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        list_image_paths = self.x.iloc[idx]\n",
    "        # Opening one every 'downsample_factor' image\n",
    "        images = [PIL.Image.open(PATH_ROOT_DIR + im) \n",
    "                  for im in list_image_paths[::self.downsample_factor]]\n",
    "        target = self.y.iloc[idx]\n",
    "        \n",
    "        # Resizing\n",
    "        if self.transform:\n",
    "            images = [self.transform(im) for im in images]\n",
    "            \n",
    "        # Normalizing the .jpgs to [-0.5, 0.5] both for \n",
    "        # faces and sound STFTs.\n",
    "        # TODO: Normalize faces with imagenet stats, as the \n",
    "        # model taking faces as input is pretrained on imagenet\n",
    "        images = [(np.array(im) / 255.0) - 0.5 for im in images]\n",
    "        \n",
    "        # Adding channel dimension to STFT images (grayscale)\n",
    "        if len(images[0].shape) == 2: \n",
    "            images = [im[...,None] for im in images]\n",
    "            \n",
    "        # (n_frames, channels, height, width)    \n",
    "        return torch.Tensor(images).permute(0,3,1,2)\n",
    "    \n",
    "class DeepFakeDetectionDataset(Dataset):\n",
    "    \"\"\"DeepFakeDetectionDataset. Merges faces & STFT datasets.\"\"\"\n",
    "    def __init__(self, x1, x2, y):\n",
    "        self.x1,self.x2,self.y = x1,x2,y\n",
    "    def __len__(self): \n",
    "        return len(self.y)\n",
    "    def __getitem__(self, i): \n",
    "        return (self.x1[i], self.x2[i]), self.y[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = DeepFakeDF(root_dirs).get_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>audio</th>\n",
       "      <th>video_name</th>\n",
       "      <th>sample</th>\n",
       "      <th>label</th>\n",
       "      <th>actor_0</th>\n",
       "      <th>actor_1</th>\n",
       "      <th>dir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>train_sample_videos/face_frames/aagfhgtpmv/aud...</td>\n",
       "      <td>aagfhgtpmv</td>\n",
       "      <td>0</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>train_sample_videos/face_frames/aagfhgtpmv/web...</td>\n",
       "      <td>train_sample_videos/face_frames/aagfhgtpmv/web...</td>\n",
       "      <td>train_sample_videos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>train_sample_videos/face_frames/aagfhgtpmv/aud...</td>\n",
       "      <td>aagfhgtpmv</td>\n",
       "      <td>1</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>train_sample_videos/face_frames/aagfhgtpmv/web...</td>\n",
       "      <td>train_sample_videos/face_frames/aagfhgtpmv/web...</td>\n",
       "      <td>train_sample_videos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>train_sample_videos/face_frames/aagfhgtpmv/aud...</td>\n",
       "      <td>aagfhgtpmv</td>\n",
       "      <td>2</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>train_sample_videos/face_frames/aagfhgtpmv/web...</td>\n",
       "      <td>train_sample_videos/face_frames/aagfhgtpmv/web...</td>\n",
       "      <td>train_sample_videos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586</th>\n",
       "      <td>train_sample_videos/face_frames/aagfhgtpmv/aud...</td>\n",
       "      <td>aagfhgtpmv</td>\n",
       "      <td>3</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>train_sample_videos/face_frames/aagfhgtpmv/web...</td>\n",
       "      <td>train_sample_videos/face_frames/aagfhgtpmv/web...</td>\n",
       "      <td>train_sample_videos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>train_sample_videos/face_frames/aagfhgtpmv/aud...</td>\n",
       "      <td>aagfhgtpmv</td>\n",
       "      <td>4</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>train_sample_videos/face_frames/aagfhgtpmv/web...</td>\n",
       "      <td>train_sample_videos/face_frames/aagfhgtpmv/web...</td>\n",
       "      <td>train_sample_videos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 audio  video_name  sample  \\\n",
       "300  train_sample_videos/face_frames/aagfhgtpmv/aud...  aagfhgtpmv       0   \n",
       "301  train_sample_videos/face_frames/aagfhgtpmv/aud...  aagfhgtpmv       1   \n",
       "498  train_sample_videos/face_frames/aagfhgtpmv/aud...  aagfhgtpmv       2   \n",
       "586  train_sample_videos/face_frames/aagfhgtpmv/aud...  aagfhgtpmv       3   \n",
       "597  train_sample_videos/face_frames/aagfhgtpmv/aud...  aagfhgtpmv       4   \n",
       "\n",
       "    label                                            actor_0  \\\n",
       "300  FAKE  train_sample_videos/face_frames/aagfhgtpmv/web...   \n",
       "301  FAKE  train_sample_videos/face_frames/aagfhgtpmv/web...   \n",
       "498  FAKE  train_sample_videos/face_frames/aagfhgtpmv/web...   \n",
       "586  FAKE  train_sample_videos/face_frames/aagfhgtpmv/web...   \n",
       "597  FAKE  train_sample_videos/face_frames/aagfhgtpmv/web...   \n",
       "\n",
       "                                               actor_1                  dir  \n",
       "300  train_sample_videos/face_frames/aagfhgtpmv/web...  train_sample_videos  \n",
       "301  train_sample_videos/face_frames/aagfhgtpmv/web...  train_sample_videos  \n",
       "498  train_sample_videos/face_frames/aagfhgtpmv/web...  train_sample_videos  \n",
       "586  train_sample_videos/face_frames/aagfhgtpmv/web...  train_sample_videos  \n",
       "597  train_sample_videos/face_frames/aagfhgtpmv/web...  train_sample_videos  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset format - indexed on frames, simple use case where predictions \n",
    "# are made independently on each frame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_name</th>\n",
       "      <th>audio</th>\n",
       "      <th>actor_0</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aagfhgtpmv</td>\n",
       "      <td>[train_sample_videos/face_frames/aagfhgtpmv/au...</td>\n",
       "      <td>[train_sample_videos/face_frames/aagfhgtpmv/we...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aapnvogymq</td>\n",
       "      <td>[train_sample_videos/face_frames/aapnvogymq/au...</td>\n",
       "      <td>[train_sample_videos/face_frames/aapnvogymq/we...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abarnvbtwb</td>\n",
       "      <td>[train_sample_videos/face_frames/abarnvbtwb/au...</td>\n",
       "      <td>[train_sample_videos/face_frames/abarnvbtwb/we...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abofeumbvv</td>\n",
       "      <td>[train_sample_videos/face_frames/abofeumbvv/au...</td>\n",
       "      <td>[train_sample_videos/face_frames/abofeumbvv/we...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abqwwspghj</td>\n",
       "      <td>[train_sample_videos/face_frames/abqwwspghj/au...</td>\n",
       "      <td>[train_sample_videos/face_frames/abqwwspghj/we...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   video_name                                              audio  \\\n",
       "0  aagfhgtpmv  [train_sample_videos/face_frames/aagfhgtpmv/au...   \n",
       "1  aapnvogymq  [train_sample_videos/face_frames/aapnvogymq/au...   \n",
       "2  abarnvbtwb  [train_sample_videos/face_frames/abarnvbtwb/au...   \n",
       "3  abofeumbvv  [train_sample_videos/face_frames/abofeumbvv/au...   \n",
       "4  abqwwspghj  [train_sample_videos/face_frames/abqwwspghj/au...   \n",
       "\n",
       "                                             actor_0 label  \n",
       "0  [train_sample_videos/face_frames/aagfhgtpmv/we...  FAKE  \n",
       "1  [train_sample_videos/face_frames/aapnvogymq/we...  FAKE  \n",
       "2  [train_sample_videos/face_frames/abarnvbtwb/we...  REAL  \n",
       "3  [train_sample_videos/face_frames/abofeumbvv/we...  FAKE  \n",
       "4  [train_sample_videos/face_frames/abqwwspghj/we...  FAKE  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset format - indexed on videos, use case where all the frames of a \n",
    "# video are used to make a TRUE/FAKE prediction.\n",
    "# Each video is assigned a list of all its extracted faces and all stfts.\n",
    "gb = df.groupby('video_name')\n",
    "audio = gb['audio'].apply(list)\n",
    "video = gb['actor_0'].apply(list)\n",
    "label = gb['label'].nth(0)\n",
    "df = pd.concat([audio,video,label],axis=1)\n",
    "df.reset_index(inplace = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train/val split indices\n",
    "val_perc = 0.2\n",
    "n_val = int(val_perc*len(df))\n",
    "shuffled_idx = np.random.permutation(df.index.tolist())\n",
    "\n",
    "val_idx = shuffled_idx[:n_val]\n",
    "train_idx = shuffled_idx[n_val:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsample_factor = 15 # using one every 15 frames only\n",
    "n_frames = 300 // downsample_factor\n",
    "\n",
    "# train/val torch datasets & dataloaders\n",
    "tr_images = DeepFakeJPGDataset(df.iloc[train_idx].reset_index(drop = True), \n",
    "                               'actor_0', transforms.Resize((150,100)), downsample_factor)\n",
    "tr_sound = DeepFakeJPGDataset(df.iloc[train_idx].reset_index(drop = True),\n",
    "                               'audio', transforms.Resize((65,25)), downsample_factor)\n",
    "train_ds = DeepFakeDetectionDataset(tr_images, tr_sound, tr_images.y)\n",
    "train_dl = DataLoader(train_ds)\n",
    "\n",
    "val_images = DeepFakeJPGDataset(df.iloc[val_idx].reset_index(drop = True), \n",
    "                                'actor_0', transforms.Resize((150,100)), downsample_factor)\n",
    "val_sound = DeepFakeJPGDataset(df.iloc[val_idx].reset_index(drop = True), \n",
    "                               'audio', transforms.Resize((65,25)), downsample_factor)\n",
    "valid_ds = DeepFakeDetectionDataset(val_images, val_sound, val_images.y)\n",
    "valid_dl = DataLoader(valid_ds)\n",
    "\n",
    "# fastai databunch\n",
    "db = DataBunch(train_dl,valid_dl)\n",
    "db.batch_size = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "# Models #\n",
    "##########\n",
    "\n",
    "# Frame embeddings\n",
    "def fully_connected(layers, dropout, bn = True):\n",
    "    \"\"\"Returns a series of [BatchNorm1d, Dropout, Linear]*len(layers)\n",
    "    The size of the linear layers is given by 'layers'. \"\"\"\n",
    "    model_layers = [] \n",
    "    activations = [nn.ReLU(inplace=True)] * (len(layers)-1)\n",
    "    for n_in, n_out, p, actn in zip(layers[:-1], layers[1:], dropout, activations):\n",
    "        model_layers += bn_drop_lin(n_in, n_out, p = p, actn = actn, bn = bn)\n",
    "    return nn.Sequential(*model_layers)\n",
    "\n",
    "class DoubleSelfAttention(nn.Module):\n",
    "    \"\"\"Applies attention in the frame embedding space and in the time space\"\"\"\n",
    "    def __init__(self, frame_embedding_size, n_frames): \n",
    "        super().__init__()\n",
    "        self.self_attention = fastai.layers.SelfAttention(n_frames)\n",
    "        self.self_attention_T = fastai.layers.SelfAttention(frame_embedding_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.self_attention(x)\n",
    "        x = x.permute(0,2,1)\n",
    "        x = self.self_attention_T(x)\n",
    "        x = x.permute(0,2,1)\n",
    "        return x\n",
    "\n",
    "class VideoAnalyzer(nn.Module):\n",
    "    def __init__(self, frame_embedding_size, n_frames): \n",
    "        super().__init__()\n",
    "        self.self_attention = DoubleSelfAttention(frame_embedding_size,n_frames)\n",
    "        layers = [n_frames*frame_embedding_size, n_frames*frame_embedding_size // 2, 50]\n",
    "        self.linears = fully_connected(layers, [0.1]*len(layers))\n",
    "        self.classifier = nn.Linear(layers[-1],2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.self_attention(x)\n",
    "        x = x.contiguous().view(x.size(0), -1)\n",
    "        x = self.linears(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "# Global model\n",
    "class DeepFakeDetector(nn.Module):\n",
    "    \"\"\"DeepFakeDetectionModel. This model has four main parts:\n",
    "    -Face analyzer: convnet 2d pretrained on imagenet.\n",
    "    -STFT analyzer: convnet 2d.\n",
    "    -Face & STFT merger: fully connected network, takes the output of the two above networks\n",
    "        as input, and outputs a vector (small dimension) representation of the frame's video and\n",
    "        audio - frame embeddings.\n",
    "    -Video analyzer: once the three above network have processed all the frames of a video, \n",
    "        the concatenation of the frame embeddings is passed to a 4th network. This network sees\n",
    "        the entire video through its frame embeddings, and predicts the label TRUE/FAKE.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_face, model_stft, model_merge, model_video, n_frames): \n",
    "        super().__init__()\n",
    "        self.n_frames = n_frames\n",
    "        \n",
    "        # two conv heads\n",
    "        self.model_face = model_face\n",
    "        self.model_stft = model_stft\n",
    "        self.poolflat = fastai.layers.PoolFlatten()\n",
    "        \n",
    "        # frame embeddings\n",
    "        self.model_merge = model_merge\n",
    "        \n",
    "        # frame embeddings aggregator, and classifier\n",
    "        self.model_video = model_video\n",
    "\n",
    "    def forward(self, *x):\n",
    "        x_faces = x[0]\n",
    "        x_stfts = x[1]\n",
    "        \n",
    "        frame_embeddings = []\n",
    "        for frame in range(self.n_frames):\n",
    "            \n",
    "            x_face = self.model_face(x_faces[:,frame,:,:,:])\n",
    "            x_face = self.poolflat(x_face)\n",
    "            x_stft = self.model_stft(x_stfts[:,frame,:,:,:])\n",
    "            x = torch.cat([x_face, x_stft], dim=1)\n",
    "            x = self.model_merge(x)\n",
    "            frame_embeddings.append(x[:,None,:])\n",
    "        \n",
    "        x = torch.cat(frame_embeddings, dim = 1)\n",
    "        x = self.model_video(x)\n",
    "        return F.log_softmax(x, dim = -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Submodels\n",
    "model_faces = create_body(fastai.vision.models.resnet18)\n",
    "\n",
    "model_stfts = simple_cnn(actns = [1,8,16,32,64], strides = [(2,1),(2,2),(2,2),(2,2)],\n",
    "                         bn = True)\n",
    "\n",
    "frame_embedding_size = 15\n",
    "merge_layers = [512 + 64, 70, frame_embedding_size]\n",
    "model_frame = fully_connected(merge_layers, dropout = [0.1]*len(merge_layers))\n",
    "\n",
    "model_video = VideoAnalyzer(frame_embedding_size, n_frames)\n",
    "# Global model\n",
    "model = DeepFakeDetector(model_faces, model_stfts, model_frame, \n",
    "                             model_video, n_frames = n_frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeepFakeDetector(\n",
       "  (model_face): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (model_stft): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1), bias=False)\n",
       "      (1): ReLU(inplace)\n",
       "      (2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): ReLU(inplace)\n",
       "      (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): ReLU(inplace)\n",
       "      (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): ReLU(inplace)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): AdaptiveAvgPool2d(output_size=1)\n",
       "      (1): Flatten()\n",
       "    )\n",
       "  )\n",
       "  (poolflat): Sequential(\n",
       "    (0): AdaptiveAvgPool2d(output_size=1)\n",
       "    (1): Flatten()\n",
       "  )\n",
       "  (model_merge): Sequential(\n",
       "    (0): BatchNorm1d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): Dropout(p=0.1)\n",
       "    (2): Linear(in_features=576, out_features=70, bias=True)\n",
       "    (3): ReLU(inplace)\n",
       "    (4): BatchNorm1d(70, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): Dropout(p=0.1)\n",
       "    (6): Linear(in_features=70, out_features=15, bias=True)\n",
       "    (7): ReLU(inplace)\n",
       "  )\n",
       "  (model_video): VideoAnalyzer(\n",
       "    (self_attention): DoubleSelfAttention(\n",
       "      (self_attention): SelfAttention(\n",
       "        (query): Conv1d(20, 2, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (key): Conv1d(20, 2, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (value): Conv1d(20, 20, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      )\n",
       "      (self_attention_T): SelfAttention(\n",
       "        (query): Conv1d(15, 1, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (key): Conv1d(15, 1, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (value): Conv1d(15, 15, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (linears): Sequential(\n",
       "      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.1)\n",
       "      (2): Linear(in_features=300, out_features=150, bias=True)\n",
       "      (3): ReLU(inplace)\n",
       "      (4): BatchNorm1d(150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1)\n",
       "      (6): Linear(in_features=150, out_features=50, bias=True)\n",
       "      (7): ReLU(inplace)\n",
       "    )\n",
       "    (classifier): Linear(in_features=50, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(db, model, metrics = [accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x1,x2), y = next(iter(db.train_dl))\n",
    "x1 = x1.to('cuda')\n",
    "x2 = x2.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 512, 5, 4])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_faces(x1[:,0,:,:]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 64])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_stfts(x2[:,0,:,:]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn = Learner(db, model, metrics = [accuracy])\n",
    "learn.lr_find()\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 24:35 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.634254</td>\n",
       "      <td>0.603899</td>\n",
       "      <td>0.862500</td>\n",
       "      <td>04:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.589466</td>\n",
       "      <td>0.477783</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>04:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.533793</td>\n",
       "      <td>0.427244</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>04:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.466772</td>\n",
       "      <td>0.421718</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>04:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.404283</td>\n",
       "      <td>0.384244</td>\n",
       "      <td>0.887500</td>\n",
       "      <td>04:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.352450</td>\n",
       "      <td>0.387925</td>\n",
       "      <td>0.887500</td>\n",
       "      <td>04:04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(6,5*1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 13:43 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.603848</td>\n",
       "      <td>0.540501</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>01:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.405393</td>\n",
       "      <td>0.440084</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>01:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.325628</td>\n",
       "      <td>0.435164</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>01:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.304964</td>\n",
       "      <td>0.367607</td>\n",
       "      <td>0.862500</td>\n",
       "      <td>01:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.264422</td>\n",
       "      <td>0.386004</td>\n",
       "      <td>0.837500</td>\n",
       "      <td>01:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.207702</td>\n",
       "      <td>0.324583</td>\n",
       "      <td>0.862500</td>\n",
       "      <td>01:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.132341</td>\n",
       "      <td>0.315138</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>01:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.074944</td>\n",
       "      <td>0.323226</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>01:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.041357</td>\n",
       "      <td>0.328530</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>01:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.025301</td>\n",
       "      <td>0.329194</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>01:21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(10,5*1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For inference on the test set\n",
    "learn.save('5_ep_1e-3')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
