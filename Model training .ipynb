{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import fastai\n",
    "from fastai.text.models.transformer import MultiHeadAttention\n",
    "from fastai.vision import *\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "from skimage import io, transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements\n",
    "\n",
    "Data format:\n",
    "* Faces and STFTs must have been extracted with the notebook Extract faces and sound.ipynb.\n",
    "* In each video folder ('train_sample_videos', 'dfdc_train_videos_X', ...'), there must be a folder named 'face_frames' containing folders of faces and stfts for each video.\n",
    "\n",
    "Libraries:\n",
    "* pytorch\n",
    "* fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_ROOT_DIR = \"/media/dlo/New Volume/DeepFake/\"\n",
    "AUDIO_EXTENSION = \".jpg\"\n",
    "LABEL_FILE = \"metadata.json\"\n",
    "root_dirs = [\"train_sample_videos/\"]\n",
    "\n",
    "# train on all set\n",
    "#root_dirs = glob(f\"{PATH_ROOT_DIR}/*/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################\n",
    "# Data preparation (indexing of extracted faces & stft per video) #\n",
    "# Custom pytorch dataset creation                                 #\n",
    "###################################################################\n",
    "\n",
    "# Data preparation\n",
    "class DeepFakeDF():\n",
    "    \"\"\" Indexes the faces & STFTs into a dataframe, to be used\n",
    "    by the pytorch dataset. processing is a bit long\"\"\"\n",
    "    def __init__(self, data_dirs, test = False):\n",
    "        self.data_dirs = [f\"{PATH_ROOT_DIR}{d}\" for d in data_dirs]\n",
    "        self.frame_dirs = [f\"{d}face_frames/\" for d in self.data_dirs]\n",
    "        \n",
    "        audio = []\n",
    "        video = []\n",
    "        for frame_dir in self.frame_dirs:\n",
    "            audio.extend(glob.glob(f\"{frame_dir}*/audio*\"))\n",
    "            video.extend(glob.glob(f\"{frame_dir}*/webcam*\"))\n",
    "            \n",
    "        if not test:\n",
    "            self.labels = self._get_labels()\n",
    "        else:\n",
    "            self.labels = {}\n",
    "            \n",
    "        self.video = video\n",
    "        self.df = self._prep_df_audio(audio)\n",
    "        self.video_dicts = self._prep_video_dicts(video)\n",
    "        self._merge_audio_video()\n",
    "        \n",
    "    def get_df(self):\n",
    "        return self.df\n",
    "        \n",
    "    def _get_labels(self):\n",
    "        labels = {}\n",
    "        for d in self.data_dirs:\n",
    "            with open(f\"{d}{LABEL_FILE}\", \"r\") as f:\n",
    "                labels.update({f\"{k.split('.mp4')[0]}\": v['label'] \n",
    "                             for k, v in json.load(f).items()})\n",
    "                \n",
    "        return labels\n",
    "    \n",
    "    def _merge_audio_video(self):\n",
    "        \"\"\"\n",
    "        Audio has 1 sample per frame in any case. But the face extractor may have \n",
    "        missed some faces. This function attempts to provide a face for each audio sample.\n",
    "        \"\"\"\n",
    "        self.df['dir'] = self.df['audio'].str.split(\"/\").str[-4]\n",
    "        \n",
    "        # Actor 0:\n",
    "        # Flagging frames for which actor 0 was detected\n",
    "        self.df['actor_0'] = [self.video_dicts[0].get(tuple(o),np.nan) \n",
    "                              for o in self.df[['video_name', 'sample']].values.tolist()]\n",
    "        \n",
    "        # Creating path variables for frames in which actor 0 was detected\n",
    "        act0 = self.df.loc[~self.df['actor_0'].isna()].copy()\n",
    "        act0['actor_0'] = (PATH_ROOT_DIR + act0['dir'] + \"/face_frames/\" + act0['video_name'] \n",
    "                           + \"/\" + \"webcam_\" + act0['sample'].astype(str) + \"_0\" \n",
    "                           + \".jpg\")\n",
    "        self.df.loc[~self.df['actor_0'].isna(), 'actor_0'] = act0\n",
    "        \n",
    "        # Actor 1:\n",
    "        # Flagging frames for which actor 1 was detected\n",
    "        self.df['actor_1'] = [self.video_dicts[1].get(tuple(o),np.nan) \n",
    "                              for o in self.df[['video_name', 'sample']].values.tolist()]\n",
    "        # Creating path variables for frames in which actor 1 was detected\n",
    "        act1 = self.df.loc[~self.df['actor_1'].isna()].copy()\n",
    "        act1['actor_1'] = (PATH_ROOT_DIR + act1['dir'] + \"/face_frames/\" + act1['video_name'] \n",
    "                           + \"/\" + \"webcam_\" + act1['sample'].astype(str) + \"_1\" \n",
    "                           + \".jpg\")\n",
    "        self.df.loc[~self.df['actor_1'].isna(), 'actor_1'] = act1\n",
    "        \n",
    "        # Filling NaNs. Forward fill per video name, so that missing faces are replaced\n",
    "        # by the previous detected face.\n",
    "        for vid in self.df['video_name'].unique():\n",
    "            cond = (self.df['video_name'] == vid)\n",
    "            \n",
    "            self.df.loc[cond,'actor_0'] = (self.df.loc[cond,'actor_0']\n",
    "                                           .fillna(method = 'ffill')\n",
    "                                           .fillna(method = 'bfill'))\n",
    "            \n",
    "            self.df.loc[cond,'actor_1'] = (self.df.loc[cond,'actor_1']\n",
    "                                           .fillna(method = 'ffill')\n",
    "                                           .fillna(method = 'bfill'))\n",
    "        \n",
    "        # As not all videos have two actors, for now, simply copying the 1st actor into the 2nd\n",
    "        # actor field when there is only 1 actor.\n",
    "        self.df.loc[self.df['actor_1'].isna(), 'actor_1'] = self.df['actor_0']\n",
    "        \n",
    "        for col in ['audio', 'actor_0', 'actor_1']:\n",
    "            self.df[col] = self.df[col].str.replace(PATH_ROOT_DIR,\"\")\n",
    "        \n",
    "    \n",
    "    def _prep_df_audio(self, audio):\n",
    "        \"\"\"Returns a dataframe indexed on frames of videos.\n",
    "        Contains the path to each .jpg of STFTs of video frames\"\"\"\n",
    "        df = pd.DataFrame(audio, columns = ['audio'])\n",
    "        df['video_name'] = df['audio'].str.split(\"/\").str[-2]\n",
    "        df['sample'] = df['audio'].str.split(\"/\").str[-1].str.split(\".\").str[0].str.split(\"_\").str[-1].astype(int)\n",
    "        df['label'] = df['video_name'].apply(lambda x: self.labels.get(x,\"\"))\n",
    "        df.sort_values(by=['video_name','sample'], inplace = True)\n",
    "        df['actor_0'] = \"\"\n",
    "        df['actor_1'] = \"\"\n",
    "        return df\n",
    "    \n",
    "    def _prep_video_dicts(self, video):\n",
    "        \"\"\"Returns dicts, one that tell if a face was detected in frames of videos,\n",
    "        and one that tells if a second face was detected in frames of videos.\"\"\"\n",
    "        video_name, frame_name = zip(*[o.split(\"/\")[-2:] for o in video])\n",
    "        samples, actors = zip(*[o.replace(\".jpg\",\"\").split(\"_\")[-2:] for o in frame_name])\n",
    "        samples = [int(o) for o in samples]\n",
    "        actors = [int(o) for o in actors]\n",
    "        actor_0_present = {(v, s) : a for v, s, a in zip(video_name, samples, actors) if a == 0}\n",
    "        actor_1_present = {(v, s) : a for v, s, a in zip(video_name, samples, actors) if a == 1}\n",
    "        return actor_0_present, actor_1_present\n",
    "\n",
    "    \n",
    "# Custom pytorch dataset\n",
    "class DeepFakeJPGDataset(Dataset):\n",
    "    \"\"\"DeepFakeJPGDataset. Opens .jpgs of either faces or STFTs for each frame of video.\n",
    "    Returns tensors of a concatenatetion of the video's frames.\"\"\"\n",
    "\n",
    "    def __init__(self, df, col_name, transform = None, downsample_factor = 1):\n",
    "        \"\"\"df[col_name] has to contain paths to .jpg files,\n",
    "        either of faces or of STFTs.\n",
    "        transform: resize images - all cropped faces don't have the same\n",
    "        shape, they won't fit together in a batch. Need to resize them,\n",
    "        use transforms.Resize((150,100)) for example. \n",
    "        downsample_factor: use > 1 to not use all the frames of a video\n",
    "        \"\"\"\n",
    "        self.x = df[col_name]\n",
    "        self.y = df['label'].astype('category').cat.codes.astype(int)\n",
    "        self.transform = transform\n",
    "        self.downsample_factor = int(downsample_factor)\n",
    "        self.col_name = col_name\n",
    "\n",
    "    def __len__(self): return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        list_image_paths = self.x.iloc[idx]\n",
    "        # Opening one every 'downsample_factor' image\n",
    "        images = [PIL.Image.open(PATH_ROOT_DIR + im) \n",
    "                  for im in list_image_paths[::self.downsample_factor]]\n",
    "        target = self.y.iloc[idx]\n",
    "        \n",
    "        # Resizing\n",
    "        if self.transform:\n",
    "            images = [self.transform(im) for im in images]\n",
    "            \n",
    "        # Normalizing the .jpgs to [-0.5, 0.5] both for \n",
    "        # faces and sound STFTs.\n",
    "        # TODO: Normalize faces with imagenet stats, as the \n",
    "        # model taking faces as input is pretrained on imagenet\n",
    "        images = [(np.array(im) / 255.0) - 0.5 for im in images]\n",
    "        \n",
    "        # Adding channel dimension to STFT images (grayscale)\n",
    "        if len(images[0].shape) == 2: \n",
    "            images = [im[...,None] for im in images]\n",
    "            \n",
    "        # (n_frames, channels, height, width)    \n",
    "        return torch.Tensor(images).permute(0,3,1,2)\n",
    "    \n",
    "class DeepFakeDetectionDataset(Dataset):\n",
    "    \"\"\"DeepFakeDetectionDataset. Merges faces & STFT datasets.\"\"\"\n",
    "    def __init__(self, x1, x2, y):\n",
    "        self.x1,self.x2,self.y = x1,x2,y\n",
    "    def __len__(self): \n",
    "        return len(self.y)\n",
    "    def __getitem__(self, i): \n",
    "        return (self.x1[i], self.x2[i]), self.y[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = DeepFakeDF(root_dirs).get_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>audio</th>\n",
       "      <th>video_name</th>\n",
       "      <th>sample</th>\n",
       "      <th>label</th>\n",
       "      <th>actor_0</th>\n",
       "      <th>actor_1</th>\n",
       "      <th>dir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>train_sample_videos/face_frames/aagfhgtpmv/aud...</td>\n",
       "      <td>aagfhgtpmv</td>\n",
       "      <td>0</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>train_sample_videos/face_frames/aagfhgtpmv/web...</td>\n",
       "      <td>train_sample_videos/face_frames/aagfhgtpmv/web...</td>\n",
       "      <td>train_sample_videos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>train_sample_videos/face_frames/aagfhgtpmv/aud...</td>\n",
       "      <td>aagfhgtpmv</td>\n",
       "      <td>1</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>train_sample_videos/face_frames/aagfhgtpmv/web...</td>\n",
       "      <td>train_sample_videos/face_frames/aagfhgtpmv/web...</td>\n",
       "      <td>train_sample_videos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>train_sample_videos/face_frames/aagfhgtpmv/aud...</td>\n",
       "      <td>aagfhgtpmv</td>\n",
       "      <td>2</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>train_sample_videos/face_frames/aagfhgtpmv/web...</td>\n",
       "      <td>train_sample_videos/face_frames/aagfhgtpmv/web...</td>\n",
       "      <td>train_sample_videos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586</th>\n",
       "      <td>train_sample_videos/face_frames/aagfhgtpmv/aud...</td>\n",
       "      <td>aagfhgtpmv</td>\n",
       "      <td>3</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>train_sample_videos/face_frames/aagfhgtpmv/web...</td>\n",
       "      <td>train_sample_videos/face_frames/aagfhgtpmv/web...</td>\n",
       "      <td>train_sample_videos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>train_sample_videos/face_frames/aagfhgtpmv/aud...</td>\n",
       "      <td>aagfhgtpmv</td>\n",
       "      <td>4</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>train_sample_videos/face_frames/aagfhgtpmv/web...</td>\n",
       "      <td>train_sample_videos/face_frames/aagfhgtpmv/web...</td>\n",
       "      <td>train_sample_videos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 audio  video_name  sample  \\\n",
       "300  train_sample_videos/face_frames/aagfhgtpmv/aud...  aagfhgtpmv       0   \n",
       "301  train_sample_videos/face_frames/aagfhgtpmv/aud...  aagfhgtpmv       1   \n",
       "498  train_sample_videos/face_frames/aagfhgtpmv/aud...  aagfhgtpmv       2   \n",
       "586  train_sample_videos/face_frames/aagfhgtpmv/aud...  aagfhgtpmv       3   \n",
       "597  train_sample_videos/face_frames/aagfhgtpmv/aud...  aagfhgtpmv       4   \n",
       "\n",
       "    label                                            actor_0  \\\n",
       "300  FAKE  train_sample_videos/face_frames/aagfhgtpmv/web...   \n",
       "301  FAKE  train_sample_videos/face_frames/aagfhgtpmv/web...   \n",
       "498  FAKE  train_sample_videos/face_frames/aagfhgtpmv/web...   \n",
       "586  FAKE  train_sample_videos/face_frames/aagfhgtpmv/web...   \n",
       "597  FAKE  train_sample_videos/face_frames/aagfhgtpmv/web...   \n",
       "\n",
       "                                               actor_1                  dir  \n",
       "300  train_sample_videos/face_frames/aagfhgtpmv/web...  train_sample_videos  \n",
       "301  train_sample_videos/face_frames/aagfhgtpmv/web...  train_sample_videos  \n",
       "498  train_sample_videos/face_frames/aagfhgtpmv/web...  train_sample_videos  \n",
       "586  train_sample_videos/face_frames/aagfhgtpmv/web...  train_sample_videos  \n",
       "597  train_sample_videos/face_frames/aagfhgtpmv/web...  train_sample_videos  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset format - indexed on frames, simple use case where predictions \n",
    "# are made independently on each frame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_name</th>\n",
       "      <th>audio</th>\n",
       "      <th>actor_0</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aagfhgtpmv</td>\n",
       "      <td>[train_sample_videos/face_frames/aagfhgtpmv/au...</td>\n",
       "      <td>[train_sample_videos/face_frames/aagfhgtpmv/we...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aapnvogymq</td>\n",
       "      <td>[train_sample_videos/face_frames/aapnvogymq/au...</td>\n",
       "      <td>[train_sample_videos/face_frames/aapnvogymq/we...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abarnvbtwb</td>\n",
       "      <td>[train_sample_videos/face_frames/abarnvbtwb/au...</td>\n",
       "      <td>[train_sample_videos/face_frames/abarnvbtwb/we...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abofeumbvv</td>\n",
       "      <td>[train_sample_videos/face_frames/abofeumbvv/au...</td>\n",
       "      <td>[train_sample_videos/face_frames/abofeumbvv/we...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abqwwspghj</td>\n",
       "      <td>[train_sample_videos/face_frames/abqwwspghj/au...</td>\n",
       "      <td>[train_sample_videos/face_frames/abqwwspghj/we...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   video_name                                              audio  \\\n",
       "0  aagfhgtpmv  [train_sample_videos/face_frames/aagfhgtpmv/au...   \n",
       "1  aapnvogymq  [train_sample_videos/face_frames/aapnvogymq/au...   \n",
       "2  abarnvbtwb  [train_sample_videos/face_frames/abarnvbtwb/au...   \n",
       "3  abofeumbvv  [train_sample_videos/face_frames/abofeumbvv/au...   \n",
       "4  abqwwspghj  [train_sample_videos/face_frames/abqwwspghj/au...   \n",
       "\n",
       "                                             actor_0 label  \n",
       "0  [train_sample_videos/face_frames/aagfhgtpmv/we...  FAKE  \n",
       "1  [train_sample_videos/face_frames/aapnvogymq/we...  FAKE  \n",
       "2  [train_sample_videos/face_frames/abarnvbtwb/we...  REAL  \n",
       "3  [train_sample_videos/face_frames/abofeumbvv/we...  FAKE  \n",
       "4  [train_sample_videos/face_frames/abqwwspghj/we...  FAKE  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset format - indexed on videos, use case where all the frames of a \n",
    "# video are used to make a TRUE/FAKE prediction.\n",
    "# Each video is assigned a list of all its extracted faces and all stfts.\n",
    "gb = df.groupby('video_name')\n",
    "audio = gb['audio'].apply(list)\n",
    "video = gb['actor_0'].apply(list)\n",
    "label = gb['label'].nth(0)\n",
    "df = pd.concat([audio,video,label],axis=1)\n",
    "df.reset_index(inplace = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train/val split indices\n",
    "val_perc = 0.2\n",
    "n_val = int(val_perc*len(df))\n",
    "shuffled_idx = np.random.permutation(df.index.tolist())\n",
    "\n",
    "val_idx = shuffled_idx[:n_val]\n",
    "train_idx = shuffled_idx[n_val:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsample_factor = 10 # using one every 10 frames only\n",
    "n_frames = 300 // downsample_factor\n",
    "\n",
    "# train/val torch datasets & dataloaders\n",
    "tr_images = DeepFakeJPGDataset(df.iloc[train_idx].reset_index(drop = True), \n",
    "                               'actor_0', transforms.Resize((150,100)), downsample_factor)\n",
    "tr_sound = DeepFakeJPGDataset(df.iloc[train_idx].reset_index(drop = True),\n",
    "                               'audio', transforms.Resize((65,25)), downsample_factor)\n",
    "train_ds = DeepFakeDetectionDataset(tr_images, tr_sound, tr_images.y)\n",
    "train_dl = DataLoader(train_ds)\n",
    "\n",
    "val_images = DeepFakeJPGDataset(df.iloc[val_idx].reset_index(drop = True), \n",
    "                                'actor_0', transforms.Resize((150,100)), downsample_factor)\n",
    "val_sound = DeepFakeJPGDataset(df.iloc[val_idx].reset_index(drop = True), \n",
    "                               'audio', transforms.Resize((65,25)), downsample_factor)\n",
    "valid_ds = DeepFakeDetectionDataset(val_images, val_sound, val_images.y)\n",
    "valid_dl = DataLoader(valid_ds)\n",
    "\n",
    "# fastai databunch\n",
    "db = DataBunch(train_dl,valid_dl)\n",
    "db.batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "# Models #\n",
    "##########\n",
    "\n",
    "# Frame embeddings\n",
    "def fully_connected(layers, dropout, bn = True):\n",
    "    \"\"\"Returns a series of [BatchNorm1d, Dropout, Linear]*len(layers)\n",
    "    The size of the linear layers is given by 'layers'. \"\"\"\n",
    "    model_layers = [] \n",
    "    activations = [nn.ReLU(inplace=True)] * (len(layers)-1)\n",
    "    for n_in, n_out, p, actn in zip(layers[:-1], layers[1:], dropout, activations):\n",
    "        model_layers += bn_drop_lin(n_in, n_out, p = p, actn = actn, bn = bn)\n",
    "    return nn.Sequential(*model_layers)\n",
    "\n",
    "class DoubleSelfAttention(nn.Module):\n",
    "    \"\"\"Applies attention in the frame embedding space and in the time space\"\"\"\n",
    "    def __init__(self, frame_embedding_size, n_frames): \n",
    "        super().__init__()\n",
    "        self.self_attention = fastai.layers.SelfAttention(n_frames)\n",
    "        self.self_attention_T = fastai.layers.SelfAttention(frame_embedding_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.self_attention(x)\n",
    "        x = x.permute(0,2,1)\n",
    "        x = self.self_attention_T(x)\n",
    "        x = x.permute(0,2,1)\n",
    "        return x\n",
    "\n",
    "class VideoAnalyzer(nn.Module):\n",
    "    def __init__(self, frame_embedding_size, n_frames): \n",
    "        super().__init__()\n",
    "        self.self_attention = MultiHeadAttention(5,frame_embedding_size)\n",
    "        layers = [n_frames*frame_embedding_size, n_frames*frame_embedding_size // 2, 50]\n",
    "        self.linears = fully_connected(layers, [0.1]*len(layers))\n",
    "        self.classifier = nn.Linear(layers[-1],2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.self_attention(x)\n",
    "        x = x.contiguous().view(x.size(0), -1)\n",
    "        x = self.linears(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "# Global model\n",
    "class DeepFakeDetector(nn.Module):\n",
    "    \"\"\"DeepFakeDetectionModel. This model has four main parts:\n",
    "    -Face analyzer: convnet 2d pretrained on imagenet.\n",
    "    -STFT analyzer: convnet 2d.\n",
    "    -Face & STFT merger: fully connected network, takes the output of the two above networks\n",
    "        as input, and outputs a vector (small dimension) representation of the frame's video and\n",
    "        audio - frame embeddings.\n",
    "    -Video analyzer: once the three above network have processed all the frames of a video, \n",
    "        the concatenation of the frame embeddings is passed to a 4th network. This network sees\n",
    "        the entire video through its frame embeddings, and predicts the label TRUE/FAKE.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_face, model_stft, model_merge, model_video, n_frames): \n",
    "        super().__init__()\n",
    "        self.n_frames = n_frames\n",
    "        \n",
    "        # two conv heads\n",
    "        self.model_face = model_face\n",
    "        self.model_stft = model_stft\n",
    "        self.poolflat = fastai.layers.PoolFlatten()\n",
    "        \n",
    "        # frame embeddings\n",
    "        self.model_merge = model_merge\n",
    "        \n",
    "        # frame embeddings aggregator, and classifier\n",
    "        self.model_video = model_video\n",
    "\n",
    "    def forward(self, *x):\n",
    "        x_faces = x[0]\n",
    "        x_stfts = x[1]\n",
    "        \n",
    "        frame_embeddings = []\n",
    "        for frame in range(self.n_frames):\n",
    "            \n",
    "            x_face = self.model_face(x_faces[:,frame,:,:,:])\n",
    "            x_face = self.poolflat(x_face)\n",
    "            x_stft = self.model_stft(x_stfts[:,frame,:,:,:])\n",
    "            x = torch.cat([x_face, x_stft], dim=1)\n",
    "            x = self.model_merge(x)\n",
    "            frame_embeddings.append(x[:,None,:])\n",
    "        \n",
    "        x = torch.cat(frame_embeddings, dim = 1)\n",
    "        x = self.model_video(x)\n",
    "        return F.log_softmax(x, dim = -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Submodels\n",
    "model_faces = create_body(fastai.vision.models.resnet18)\n",
    "\n",
    "model_stfts = simple_cnn(actns = [1,8,16,32,64], strides = [(2,1),(2,2),(2,2),(2,2)],\n",
    "                         bn = True)\n",
    "\n",
    "frame_embedding_size = 32\n",
    "merge_layers = [512 + 64, frame_embedding_size]\n",
    "model_frame = fully_connected(merge_layers, dropout = [0.1]*len(merge_layers))\n",
    "\n",
    "model_video = VideoAnalyzer(frame_embedding_size, n_frames)\n",
    "# Global model\n",
    "model = DeepFakeDetector(model_faces, model_stfts, model_frame, \n",
    "                             model_video, n_frames = n_frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(db, model, metrics = [accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x1,x2), y = next(iter(db.train_dl))\n",
    "x1 = x1.to('cuda')\n",
    "x2 = x2.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 512, 5, 4])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_faces(x1[:,0,:,:]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 64])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_stfts(x2[:,0,:,:]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='20', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/20 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='11' class='' max='40', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      27.50% [11/40 00:58<02:33 0.8180]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(20,5*1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 13:43 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.603848</td>\n",
       "      <td>0.540501</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>01:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.405393</td>\n",
       "      <td>0.440084</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>01:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.325628</td>\n",
       "      <td>0.435164</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>01:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.304964</td>\n",
       "      <td>0.367607</td>\n",
       "      <td>0.862500</td>\n",
       "      <td>01:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.264422</td>\n",
       "      <td>0.386004</td>\n",
       "      <td>0.837500</td>\n",
       "      <td>01:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.207702</td>\n",
       "      <td>0.324583</td>\n",
       "      <td>0.862500</td>\n",
       "      <td>01:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.132341</td>\n",
       "      <td>0.315138</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>01:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.074944</td>\n",
       "      <td>0.323226</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>01:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.041357</td>\n",
       "      <td>0.328530</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>01:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.025301</td>\n",
       "      <td>0.329194</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>01:21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(10,5*1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For inference on the test set\n",
    "learn.save('5_ep_1e-3')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
