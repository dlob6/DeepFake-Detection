{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import fastai\n",
    "from fastai.vision import *\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "from skimage import io, transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements\n",
    "\n",
    "Data format:\n",
    "* Faces and STFTs must have been extracted with the notebook Extract faces and sound.ipynb.\n",
    "* In each video folder ('train_sample_videos', 'dfdc_train_videos_X', ...'), there must be a folder named 'face_frames' containing folders of faces and stfts for each video.\n",
    "\n",
    "Libraries:\n",
    "* pytorch\n",
    "* fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_ROOT_DIR = \"/media/dlo/New Volume/DeepFake/\"\n",
    "AUDIO_EXTENSION = \".jpg\"\n",
    "LABEL_FILE = \"metadata.json\"\n",
    "root_dirs = [\"train_sample_videos/\"]\n",
    "\n",
    "# train on all set\n",
    "#root_dirs = glob(f\"{PATH_ROOT_DIR}/*/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################\n",
    "# Data preparation (indexing of extracted faces & stft per video) #\n",
    "# Custom pytorch dataset creation                                 #\n",
    "###################################################################\n",
    "\n",
    "# Data preparation\n",
    "class DeepFakeDF():\n",
    "    \"\"\" Indexes the faces & STFTs into a dataframe, to be used\n",
    "    by the pytorch dataset. processing is a bit long\"\"\"\n",
    "    def __init__(self, data_dirs, test = False):\n",
    "        self.data_dirs = [f\"{PATH_ROOT_DIR}{d}\" for d in data_dirs]\n",
    "        self.frame_dirs = [f\"{d}face_frames/\" for d in self.data_dirs]\n",
    "        \n",
    "        audio = []\n",
    "        video = []\n",
    "        for frame_dir in self.frame_dirs:\n",
    "            audio.extend(glob.glob(f\"{frame_dir}*/audio*\"))\n",
    "            video.extend(glob.glob(f\"{frame_dir}*/webcam*\"))\n",
    "            \n",
    "        if not test:\n",
    "            self.labels = self._get_labels()\n",
    "        else:\n",
    "            self.labels = {}\n",
    "            \n",
    "        self.video = video\n",
    "        self.df = self._prep_df_audio(audio)\n",
    "        self.video_dicts = self._prep_video_dicts(video)\n",
    "        self._merge_audio_video()\n",
    "        \n",
    "    def get_df(self):\n",
    "        return self.df\n",
    "        \n",
    "    def _get_labels(self):\n",
    "        labels = {}\n",
    "        for d in self.data_dirs:\n",
    "            with open(f\"{d}{LABEL_FILE}\", \"r\") as f:\n",
    "                labels.update({f\"{k.split('.mp4')[0]}\": v['label'] \n",
    "                             for k, v in json.load(f).items()})\n",
    "                \n",
    "        return labels\n",
    "    \n",
    "    def _merge_audio_video(self):\n",
    "        \"\"\"\n",
    "        Audio has 1 sample per frame in any case. But the face extractor may have \n",
    "        missed some faces. This function attempts to provide a face for each audio sample.\n",
    "        \"\"\"\n",
    "        self.df['dir'] = self.df['audio'].str.split(\"/\").str[-4]\n",
    "        \n",
    "        # Actor 0:\n",
    "        # Flagging frames for which actor 0 was detected\n",
    "        self.df['actor_0'] = [self.video_dicts[0].get(tuple(o),np.nan) \n",
    "                              for o in self.df[['video_name', 'sample']].values.tolist()]\n",
    "        \n",
    "        # Creating path variables for frames in which actor 0 was detected\n",
    "        act0 = self.df.loc[~self.df['actor_0'].isna()].copy()\n",
    "        act0['actor_0'] = (PATH_ROOT_DIR + act0['dir'] + \"/face_frames/\" + act0['video_name'] \n",
    "                           + \"/\" + \"webcam_\" + act0['sample'].astype(str) + \"_0\" \n",
    "                           + \".jpg\")\n",
    "        self.df.loc[~self.df['actor_0'].isna(), 'actor_0'] = act0\n",
    "        \n",
    "        # Actor 1:\n",
    "        # Flagging frames for which actor 1 was detected\n",
    "        self.df['actor_1'] = [self.video_dicts[1].get(tuple(o),np.nan) \n",
    "                              for o in self.df[['video_name', 'sample']].values.tolist()]\n",
    "        # Creating path variables for frames in which actor 1 was detected\n",
    "        act1 = self.df.loc[~self.df['actor_1'].isna()].copy()\n",
    "        act1['actor_1'] = (PATH_ROOT_DIR + act1['dir'] + \"/face_frames/\" + act1['video_name'] \n",
    "                           + \"/\" + \"webcam_\" + act1['sample'].astype(str) + \"_1\" \n",
    "                           + \".jpg\")\n",
    "        self.df.loc[~self.df['actor_1'].isna(), 'actor_1'] = act1\n",
    "        \n",
    "        # Filling NaNs. Forward fill per video name, so that missing faces are replaced\n",
    "        # by the previous detected face.\n",
    "        for vid in self.df['video_name'].unique():\n",
    "            cond = (self.df['video_name'] == vid)\n",
    "            \n",
    "            self.df.loc[cond,'actor_0'] = (self.df.loc[cond,'actor_0']\n",
    "                                           .fillna(method = 'ffill')\n",
    "                                           .fillna(method = 'bfill'))\n",
    "            \n",
    "            self.df.loc[cond,'actor_1'] = (self.df.loc[cond,'actor_1']\n",
    "                                           .fillna(method = 'ffill')\n",
    "                                           .fillna(method = 'bfill'))\n",
    "        \n",
    "        # As not all videos have two actors, for now, simply copying the 1st actor into the 2nd\n",
    "        # actor field when there is only 1 actor.\n",
    "        self.df.loc[self.df['actor_1'].isna(), 'actor_1'] = self.df['actor_0']\n",
    "        \n",
    "        for col in ['audio', 'actor_0', 'actor_1']:\n",
    "            self.df[col] = self.df[col].str.replace(PATH_ROOT_DIR,\"\")\n",
    "        \n",
    "    \n",
    "    def _prep_df_audio(self, audio):\n",
    "        \"\"\"Returns a dataframe indexed on frames of videos.\n",
    "        Contains the path to each .jpg of STFTs of video frames\"\"\"\n",
    "        df = pd.DataFrame(audio, columns = ['audio'])\n",
    "        df['video_name'] = df['audio'].str.split(\"/\").str[-2]\n",
    "        df['sample'] = df['audio'].str.split(\"/\").str[-1].str.split(\".\").str[0].str.split(\"_\").str[-1].astype(int)\n",
    "        df['label'] = df['video_name'].apply(lambda x: self.labels.get(x,\"\"))\n",
    "        df.sort_values(by=['video_name','sample'], inplace = True)\n",
    "        df['actor_0'] = \"\"\n",
    "        df['actor_1'] = \"\"\n",
    "        return df\n",
    "    \n",
    "    def _prep_video_dicts(self, video):\n",
    "        \"\"\"Returns dicts, one that tell if a face was detected in frames of videos,\n",
    "        and one that tells if a second face was detected in frames of videos.\"\"\"\n",
    "        video_name, frame_name = zip(*[o.split(\"/\")[-2:] for o in video])\n",
    "        samples, actors = zip(*[o.replace(\".jpg\",\"\").split(\"_\")[-2:] for o in frame_name])\n",
    "        samples = [int(o) for o in samples]\n",
    "        actors = [int(o) for o in actors]\n",
    "        actor_0_present = {(v, s) : a for v, s, a in zip(video_name, samples, actors) if a == 0}\n",
    "        actor_1_present = {(v, s) : a for v, s, a in zip(video_name, samples, actors) if a == 1}\n",
    "        return actor_0_present, actor_1_present\n",
    "\n",
    "    \n",
    "# Custom pytorch dataset\n",
    "class DeepFakeJPGDataset(Dataset):\n",
    "    \"\"\"DeepFakeJPGDataset. Opens .jpgs of either faces or STFTs for each frame of video.\n",
    "    Returns tensors of a concatenatetion of the video's frames.\"\"\"\n",
    "\n",
    "    def __init__(self, df, col_name, transform = None, downsample_factor = 1):\n",
    "        \"\"\"df[col_name] has to contain paths to .jpg files,\n",
    "        either of faces or of STFTs.\n",
    "        transform: resize images - all cropped faces don't have the same\n",
    "        shape, they won't fit together in a batch. Need to resize them,\n",
    "        use transforms.Resize((150,100)) for example. \n",
    "        downsample_factor: use > 1 to not use all the frames of a video\n",
    "        \"\"\"\n",
    "        self.x = df[col_name]\n",
    "        self.y = df['label'].astype('category').cat.codes.astype(int)\n",
    "        self.transform = transform\n",
    "        self.downsample_factor = int(downsample_factor)\n",
    "        self.col_name = col_name\n",
    "\n",
    "    def __len__(self): return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        list_image_paths = self.x.iloc[idx]\n",
    "        # Opening one every 'downsample_factor' image\n",
    "        images = [PIL.Image.open(PATH_ROOT_DIR + im) \n",
    "                  for im in list_image_paths[::self.downsample_factor]]\n",
    "        target = self.y.iloc[idx]\n",
    "        \n",
    "        # Resizing\n",
    "        if self.transform:\n",
    "            images = [self.transform(im) for im in images]\n",
    "            \n",
    "        # Normalizing the .jpgs to [-0.5, 0.5] both for \n",
    "        # faces and sound STFTs.\n",
    "        # TODO: Normalize faces with imagenet stats, as the \n",
    "        # model taking faces as input is pretrained on imagenet\n",
    "        images = [(np.array(im) / 255.0) - 0.5 for im in images]\n",
    "        \n",
    "        # Adding channel dimension to STFT images (grayscale)\n",
    "        if len(images[0].shape) == 2: \n",
    "            images = [im[...,None] for im in images]\n",
    "            \n",
    "        # (n_frames, channels, height, width)    \n",
    "        return torch.Tensor(images).permute(0,3,1,2)\n",
    "    \n",
    "class DeepFakeDetectionDataset(Dataset):\n",
    "    \"\"\"DeepFakeDetectionDataset. Merges faces & STFT datasets.\"\"\"\n",
    "    def __init__(self, x1, x2, y):\n",
    "        self.x1,self.x2,self.y = x1,x2,y\n",
    "    def __len__(self): return len(self.y)\n",
    "    def __getitem__(self, i): \n",
    "        return (self.x1[i], self.x2[i]), self.y[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = DeepFakeDF(root_dirs).get_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>audio</th>\n",
       "      <th>video_name</th>\n",
       "      <th>sample</th>\n",
       "      <th>label</th>\n",
       "      <th>actor_0</th>\n",
       "      <th>actor_1</th>\n",
       "      <th>dir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>train_sample_videos/face_frames/aagfhgtpmv/aud...</td>\n",
       "      <td>aagfhgtpmv</td>\n",
       "      <td>0</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>train_sample_videos/face_frames/aagfhgtpmv/web...</td>\n",
       "      <td>train_sample_videos/face_frames/aagfhgtpmv/web...</td>\n",
       "      <td>train_sample_videos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>train_sample_videos/face_frames/aagfhgtpmv/aud...</td>\n",
       "      <td>aagfhgtpmv</td>\n",
       "      <td>1</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>train_sample_videos/face_frames/aagfhgtpmv/web...</td>\n",
       "      <td>train_sample_videos/face_frames/aagfhgtpmv/web...</td>\n",
       "      <td>train_sample_videos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>train_sample_videos/face_frames/aagfhgtpmv/aud...</td>\n",
       "      <td>aagfhgtpmv</td>\n",
       "      <td>2</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>train_sample_videos/face_frames/aagfhgtpmv/web...</td>\n",
       "      <td>train_sample_videos/face_frames/aagfhgtpmv/web...</td>\n",
       "      <td>train_sample_videos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586</th>\n",
       "      <td>train_sample_videos/face_frames/aagfhgtpmv/aud...</td>\n",
       "      <td>aagfhgtpmv</td>\n",
       "      <td>3</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>train_sample_videos/face_frames/aagfhgtpmv/web...</td>\n",
       "      <td>train_sample_videos/face_frames/aagfhgtpmv/web...</td>\n",
       "      <td>train_sample_videos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>train_sample_videos/face_frames/aagfhgtpmv/aud...</td>\n",
       "      <td>aagfhgtpmv</td>\n",
       "      <td>4</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>train_sample_videos/face_frames/aagfhgtpmv/web...</td>\n",
       "      <td>train_sample_videos/face_frames/aagfhgtpmv/web...</td>\n",
       "      <td>train_sample_videos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 audio  video_name  sample  \\\n",
       "300  train_sample_videos/face_frames/aagfhgtpmv/aud...  aagfhgtpmv       0   \n",
       "301  train_sample_videos/face_frames/aagfhgtpmv/aud...  aagfhgtpmv       1   \n",
       "498  train_sample_videos/face_frames/aagfhgtpmv/aud...  aagfhgtpmv       2   \n",
       "586  train_sample_videos/face_frames/aagfhgtpmv/aud...  aagfhgtpmv       3   \n",
       "597  train_sample_videos/face_frames/aagfhgtpmv/aud...  aagfhgtpmv       4   \n",
       "\n",
       "    label                                            actor_0  \\\n",
       "300  FAKE  train_sample_videos/face_frames/aagfhgtpmv/web...   \n",
       "301  FAKE  train_sample_videos/face_frames/aagfhgtpmv/web...   \n",
       "498  FAKE  train_sample_videos/face_frames/aagfhgtpmv/web...   \n",
       "586  FAKE  train_sample_videos/face_frames/aagfhgtpmv/web...   \n",
       "597  FAKE  train_sample_videos/face_frames/aagfhgtpmv/web...   \n",
       "\n",
       "                                               actor_1                  dir  \n",
       "300  train_sample_videos/face_frames/aagfhgtpmv/web...  train_sample_videos  \n",
       "301  train_sample_videos/face_frames/aagfhgtpmv/web...  train_sample_videos  \n",
       "498  train_sample_videos/face_frames/aagfhgtpmv/web...  train_sample_videos  \n",
       "586  train_sample_videos/face_frames/aagfhgtpmv/web...  train_sample_videos  \n",
       "597  train_sample_videos/face_frames/aagfhgtpmv/web...  train_sample_videos  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset format - indexed on frames, simple use case where predictions \n",
    "# are made independently on each frame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_name</th>\n",
       "      <th>audio</th>\n",
       "      <th>actor_0</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aagfhgtpmv</td>\n",
       "      <td>[train_sample_videos/face_frames/aagfhgtpmv/au...</td>\n",
       "      <td>[train_sample_videos/face_frames/aagfhgtpmv/we...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aapnvogymq</td>\n",
       "      <td>[train_sample_videos/face_frames/aapnvogymq/au...</td>\n",
       "      <td>[train_sample_videos/face_frames/aapnvogymq/we...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abarnvbtwb</td>\n",
       "      <td>[train_sample_videos/face_frames/abarnvbtwb/au...</td>\n",
       "      <td>[train_sample_videos/face_frames/abarnvbtwb/we...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abofeumbvv</td>\n",
       "      <td>[train_sample_videos/face_frames/abofeumbvv/au...</td>\n",
       "      <td>[train_sample_videos/face_frames/abofeumbvv/we...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abqwwspghj</td>\n",
       "      <td>[train_sample_videos/face_frames/abqwwspghj/au...</td>\n",
       "      <td>[train_sample_videos/face_frames/abqwwspghj/we...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   video_name                                              audio  \\\n",
       "0  aagfhgtpmv  [train_sample_videos/face_frames/aagfhgtpmv/au...   \n",
       "1  aapnvogymq  [train_sample_videos/face_frames/aapnvogymq/au...   \n",
       "2  abarnvbtwb  [train_sample_videos/face_frames/abarnvbtwb/au...   \n",
       "3  abofeumbvv  [train_sample_videos/face_frames/abofeumbvv/au...   \n",
       "4  abqwwspghj  [train_sample_videos/face_frames/abqwwspghj/au...   \n",
       "\n",
       "                                             actor_0 label  \n",
       "0  [train_sample_videos/face_frames/aagfhgtpmv/we...  FAKE  \n",
       "1  [train_sample_videos/face_frames/aapnvogymq/we...  FAKE  \n",
       "2  [train_sample_videos/face_frames/abarnvbtwb/we...  REAL  \n",
       "3  [train_sample_videos/face_frames/abofeumbvv/we...  FAKE  \n",
       "4  [train_sample_videos/face_frames/abqwwspghj/we...  FAKE  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset format - indexed on videos, use case where all the frames of a \n",
    "# video are used to make a TRUE/FAKE prediction.\n",
    "# Each video is assigned a list of all its extracted faces and all stfts.\n",
    "gb = df.groupby('video_name')\n",
    "audio = gb['audio'].apply(list)\n",
    "video = gb['actor_0'].apply(list)\n",
    "label = gb['label'].nth(0)\n",
    "df = pd.concat([audio,video,label],axis=1)\n",
    "df.reset_index(inplace = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train/val split indices\n",
    "val_perc = 0.2\n",
    "n_val = int(val_perc*len(df))\n",
    "shuffled_idx = np.random.permutation(df.index.tolist())\n",
    "\n",
    "val_idx = shuffled_idx[:n_val]\n",
    "train_idx = shuffled_idx[n_val:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsample_factor = 30 # using one every 30 frames only\n",
    "n_frames = 300 // downsample_factor\n",
    "\n",
    "# train/val torch datasets & dataloaders\n",
    "tr_images = DeepFakeJPGDataset(df.iloc[train_idx].reset_index(drop = True), \n",
    "                               'actor_0', transforms.Resize((150,100)), downsample_factor)\n",
    "tr_sound = DeepFakeJPGDataset(df.iloc[train_idx].reset_index(drop = True),\n",
    "                               'audio', transforms.Resize((65,25)), downsample_factor)\n",
    "train_ds = DeepFakeDetectionDataset(tr_images, tr_sound, tr_images.y)\n",
    "train_dl = DataLoader(train_ds)\n",
    "\n",
    "val_images = DeepFakeJPGDataset(df.iloc[val_idx].reset_index(drop = True), \n",
    "                                'actor_0', transforms.Resize((150,100)), downsample_factor)\n",
    "val_sound = DeepFakeJPGDataset(df.iloc[val_idx].reset_index(drop = True), \n",
    "                               'audio', transforms.Resize((65,25)), downsample_factor)\n",
    "valid_ds = DeepFakeDetectionDataset(val_images, val_sound, val_images.y)\n",
    "valid_dl = DataLoader(valid_ds)\n",
    "\n",
    "# fastai databunch\n",
    "db = DataBunch(train_dl,valid_dl)\n",
    "db.batch_size = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "# Models #\n",
    "##########\n",
    "\n",
    "# Frame embeddings\n",
    "def simple_fully_connected(layers, dropout, bn = True):\n",
    "    \"\"\"Returns a series of [BatchNorm1d, Dropout, Linear]*len(layers)\n",
    "    The size of the linear layers is given by 'layers'. \"\"\"\n",
    "    model_layers = []\n",
    "    activations = [nn.ReLU(inplace=True)] * (len(layers)-1)\n",
    "    for n_in, n_out, p, actn in zip(layers[:-1], layers[1:], dropout, activations):\n",
    "        model_layers += bn_drop_lin(n_in, n_out, p = p, actn = actn, bn = bn)\n",
    "    return nn.Sequential(*model_layers)\n",
    "\n",
    "# Global model\n",
    "class DeepFakeDetector(nn.Module):\n",
    "    \"\"\"DeepFakeDetectionModel. This model has three main parts:\n",
    "    -Face analyzer: convnet 2d pretrained on imagenet.\n",
    "    -STFT analyzer: convnet 2d.\n",
    "    -Face & STFT merger: fully connected network, takes the output of the two above networks\n",
    "        as input, and outputs a vector (small dimension) representation of the frame's video and\n",
    "        audio - frame embeddings.\n",
    "    -Video analyzer: once the three above network have processed all the frames of a video, \n",
    "        the concatenation of the frame embeddings is passed to a 4th network. This network sees\n",
    "        the entire video through its frame embeddings, and predicts the label TRUE/FAKE.\n",
    "        \n",
    "    Prediction errors are backpropagated to all the subnetworks.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_face, model_stft, model_merge, model_video, n_frames): \n",
    "        super().__init__()\n",
    "        self.n_frames = n_frames\n",
    "        \n",
    "        # two conv heads\n",
    "        self.model_face = model_face\n",
    "        self.model_stft = model_stft\n",
    "        self.poolflat = fastai.layers.PoolFlatten()\n",
    "        \n",
    "        # frame embeddings\n",
    "        self.model_merge = model_merge\n",
    "        \n",
    "        # frame embeddings aggregator, and classifier\n",
    "        self.model_video = model_video\n",
    "\n",
    "    def forward(self, *x):\n",
    "        x_faces = x[0]\n",
    "        x_stfts = x[1]\n",
    "        \n",
    "        frame_embeddings = []\n",
    "        for frame in range(self.n_frames):\n",
    "            \n",
    "            x_face = self.model_face(x_faces[:,frame,:,:,:])\n",
    "            x_face = self.poolflat(x_face)\n",
    "            \n",
    "            x_stft = self.model_stft(x_stfts[:,frame,:,:,:])\n",
    "            \n",
    "            x = torch.cat([x_face, x_stft], dim=1)\n",
    "            x = self.model_merge(x)\n",
    "            \n",
    "            frame_embeddings.append(x)\n",
    "            \n",
    "        x = torch.cat(frame_embeddings, dim = 1)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.model_video(x)\n",
    "        return F.log_softmax(x, dim = -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submodels\n",
    "model_faces = create_body(fastai.vision.models.resnet18)\n",
    "\n",
    "model_stfts = simple_cnn(actns = [1,8,16,32,64], strides = [(2,1),(2,2),(2,2),(2,2)],\n",
    "                         bn = True)\n",
    "\n",
    "frame_embedding_size = 5\n",
    "merge_layers = [512 + 64, frame_embedding_size]\n",
    "model_merge_face_stfts = simple_fully_connected(merge_layers, dropout = [0.1]*len(merge_layers))\n",
    "\n",
    "model_video = nn.Linear(n_frames*merge_layers[-1],2)\n",
    "\n",
    "# Global model\n",
    "model = DeepFakeDetector(model_faces, model_stfts, model_merge_face_stfts, \n",
    "                             model_video, n_frames = n_frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(db, model, metrics = [accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x1,x2), y = next(iter(db.train_dl))\n",
    "x1 = x1.to('cuda')\n",
    "x2 = x2.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 512, 5, 4])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_faces(x1[:,0,:,:]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 64])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_stfts(x2[:,0,:,:]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5874, -0.8115],\n",
       "        [-0.6218, -0.7700],\n",
       "        [-0.5247, -0.8958],\n",
       "        [-0.4894, -0.9493],\n",
       "        [-0.6853, -0.7011],\n",
       "        [-0.6751, -0.7115],\n",
       "        [-0.6604, -0.7270],\n",
       "        [-0.5439, -0.8686]], device='cuda:0', grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x1,x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn = Learner(db, model, metrics = [accuracy])\n",
    "learn.lr_find()\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 09:52 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.649945</td>\n",
       "      <td>0.577973</td>\n",
       "      <td>0.712500</td>\n",
       "      <td>01:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.500811</td>\n",
       "      <td>0.426944</td>\n",
       "      <td>0.775000</td>\n",
       "      <td>01:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.458330</td>\n",
       "      <td>0.453005</td>\n",
       "      <td>0.775000</td>\n",
       "      <td>01:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.393657</td>\n",
       "      <td>0.464990</td>\n",
       "      <td>0.787500</td>\n",
       "      <td>01:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.292346</td>\n",
       "      <td>0.425390</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>01:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.196943</td>\n",
       "      <td>0.432047</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>01:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.137170</td>\n",
       "      <td>0.439046</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>01:24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(7,3*1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For inference on the test set\n",
    "learn.save('5_ep_1e-3')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
