{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from sys import platform\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "from PIL import Image\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import fastai\n",
    "from fastai.vision import *\n",
    "import cv2\n",
    "import imutils\n",
    "from models import *\n",
    "from utils.datasets import *\n",
    "from utils.utils import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements\n",
    "\n",
    "Yolov3: download weights from my gdrive: https://drive.google.com/open?id=1UisP4AQEW_R8scxenZ1h6tnJf9gkiq4U\n",
    "<br>\n",
    "and unzip them in weights/\n",
    "<br>\n",
    "pip install imutils<br>\n",
    "the rest is standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root folder of videos from Kaggle's deepfake contest\n",
    "PATH_ROOT_VID = \"/media/dlo/New Volume/DeepFake/\"\n",
    "\n",
    "VIDEO_EXT = '.mp4'\n",
    "AUDIO_EXT = '.wav'\n",
    "CSV_EXT = '.csv'\n",
    "\n",
    "# Config and weight files of yolov3 trained on WIDERFACE\n",
    "CFG = 'cfg/yolov3-1cls.cfg'\n",
    "WEIGHTS = 'weights/last.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER FUNCTIONS AUDIO\n",
    "\n",
    "def video_audio_text_files(video_name, path_curr_vid):\n",
    "    \"\"\"Returns path names from a video name\"\"\"\n",
    "    video_file = f\"'{path_curr_vid}{video_name}'\"\n",
    "    audio_file = video_file.replace(VIDEO_EXT,AUDIO_EXT)\n",
    "    return video_file, audio_file\n",
    "\n",
    "def numpy_from_audio(audio_file, downsample_factor = None):\n",
    "    \"\"\"\n",
    "    Reads an audio .wav file and returns its samples in a numpy array, \n",
    "    and the sampling rate [Hz]\"\"\"\n",
    "    sample_rate, samples = wavfile.read(audio_file.replace('\\'',''))\n",
    "    if downsample_factor is not None:\n",
    "        samples = signal.resample(samples, len(samples) // downsample_factor)\n",
    "        sample_rate //= downsample_factor\n",
    "    drop_samples = -(len(samples) % sample_rate)\n",
    "    return samples[:drop_samples], sample_rate\n",
    "\n",
    "def get_stft_db(samples, sample_rate):\n",
    "    \"\"\"Reads in 'audio_file', takes its STFT with a window size of 1024, \n",
    "    takes the magnitude of it, and returns its dB values.\n",
    "    \"\"\"\n",
    "    f, t, Zxx = signal.stft(samples, fs = sample_rate, nperseg = 128)\n",
    "    return f, t, np.log(np.abs(Zxx))\n",
    "\n",
    "\n",
    "def plot_time_signal(s):\n",
    "    plt.plot(s)\n",
    "    plt.ylabel('Sound Amplitude')\n",
    "    plt.xlabel('Sample')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_stft(s):\n",
    "    plt.pcolormesh(s)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xlabel('Time')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_stfts(video_name, do_plot, path_curr_vid, path_current_frames):\n",
    "    \"\"\"Creates .jpg files of the an audio signal's STFT.\n",
    "    A video 'video_name' is taken as input, \n",
    "    a temporary .wav file containing its uncompressed audio is created,\n",
    "    .jpg files containing the dB of its STFT are written to disk, 300 of them\n",
    "    (one per video frame, to match the extracted faces).\n",
    "    the temporary .wav file is then deleted.\n",
    "    \"\"\"\n",
    "    \n",
    "    video_file, audio_file = video_audio_text_files(video_name, path_curr_vid)\n",
    "    \n",
    "    \n",
    "    files_exist = {f : os.path.isfile(f.replace('\\'','')) \n",
    "                   for f in [video_file, audio_file]}\n",
    "    \n",
    "    if not files_exist[video_file]: return 0\n",
    "    \n",
    "    # Create audio_file .wav file from video\n",
    "    if not files_exist[audio_file] :\n",
    "        convert_command = f\"ffmpeg -i {video_file} {audio_file}\"\n",
    "        os.system(convert_command)\n",
    "    \n",
    "    # Read in audio file\n",
    "    raw_samples = numpy_from_audio(audio_file)\n",
    "    \n",
    "    # Plot a sample\n",
    "    if do_plot:\n",
    "        plot_time_signal(raw_samples[0])\n",
    "    # Get STFT in dB\n",
    "    _, _, samples = get_stft_db(*raw_samples)\n",
    "    \n",
    "    drop_samples = -(samples.shape[1] % 300)\n",
    "    samples = samples[:,:drop_samples]\n",
    "    \n",
    "    # Plot a STFT sample\n",
    "    if do_plot:\n",
    "        plot_stft(samples)\n",
    "        \n",
    "    samples -= np.min(samples)\n",
    "    samples /= np.max(samples)\n",
    "    \n",
    "    chunk_size = samples.shape[1] // 300\n",
    "    for chunk_idx in range(300):\n",
    "        fname = f\"{path_current_frames}audio_{chunk_idx}.jpg\"\n",
    "        if not os.path.isfile(fname):\n",
    "            Image.fromarray((samples[:,chunk_idx*chunk_size:(chunk_idx+1)*chunk_size] * 255)\n",
    "                            .astype(np.uint8)).save(fname)\n",
    "            \n",
    "    delete_command = f\"rm {audio_file}\"\n",
    "    os.system(delete_command)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### HELPER FUNCTIONS VIDEO\n",
    "\n",
    "def get_model():\n",
    "    \"\"\"Returns a Darknet model pretrained on the WIDER dataset, and\n",
    "    the device on which the model is running (CUDA hopefully)\"\"\"\n",
    "    device = torch_utils.select_device(force_cpu=ONNX_EXPORT)\n",
    "    torch.backends.cudnn.benchmark = False  # set False for reproducible results\n",
    "    \n",
    "    # Initialize model with pretrained weights\n",
    "    model = Darknet(CFG, 416)\n",
    "    model.load_state_dict(torch.load(WEIGHTS, map_location=device)['model'])\n",
    "    model.to(device).eval()\n",
    "    return model, device\n",
    "\n",
    "def get_dataloader(invid):\n",
    "    \"\"\"\n",
    "    Returns batches of image size 416\"\"\"\n",
    "    return LoadWebcam(img_size=416, half=False, pipe=invid)\n",
    "\n",
    "def write_face_samples(model, dataloader, output_path, device):\n",
    "    \"\"\"\n",
    "    Writes to disk a series of faces detected in a video sample\"\"\"\n",
    "    if os.path.isdir(output_path) : return\n",
    "    os.mkdir(output_path)\n",
    "    \n",
    "    # Confidence and non_max_suppression threshold \n",
    "    conf_thres = 0.8\n",
    "    nms_thres = 0.6\n",
    "    \n",
    "    # Looping over frames of video\n",
    "    for i, (path, img, im0, vid_cap) in enumerate(dataloader):\n",
    "        # Getting bounding box preds\n",
    "        img = torch.from_numpy(img).unsqueeze(0).to(device)\n",
    "        pred, _ = model(img)\n",
    "        \n",
    "        # Removing overlapping bounding boxes and denoising\n",
    "        det = non_max_suppression(pred.float(), conf_thres, nms_thres)[0]\n",
    "        if det is not None and len(det) > 0:\n",
    "            # Scaling bounding box coords to raw image size\n",
    "            det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n",
    "            \n",
    "            # Looping over bounding boxes, extracting faces from the video frame\n",
    "            face_ix = 0\n",
    "            for *xyxy, conf, cls_conf, cls in det:\n",
    "                (startX, startY, endX, endY) = int(xyxy[0]), int(xyxy[1]), int(xyxy[2]), int(xyxy[3])\n",
    "                face = im0[startY:endY, startX:endX]\n",
    "                \n",
    "                save_path = f\"{output_path}webcam_{i}_{face_ix}.jpg\"\n",
    "                cv2.imwrite(save_path, face)\n",
    "                face_ix += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(curr_dir):\n",
    "    \"\"\"\n",
    "    Process a directory of videos for kaggle's deepfake challenge.\n",
    "    For each video of the directory:\n",
    "    Compute a stft of the audio portion of the file, save it to disk\n",
    "    Detect the faces present in the file, save them as .jpg to disk\n",
    "    \"\"\"\n",
    "    \n",
    "    path_curr_vid = f\"{PATH_ROOT_VID}{curr_dir}/\"\n",
    "    path_write_faces = f\"{PATH_ROOT_VID}{curr_dir}/face_frames/\"\n",
    "    \n",
    "    print(f\"Reading videos from {path_curr_vid}\")\n",
    "    video_names = []\n",
    "    for filename in glob.iglob(path_curr_vid + '*.mp4', recursive=True):\n",
    "        video_names.append(filename.split(\"/\")[-1])\n",
    "    \n",
    "    if not os.path.isdir(path_write_faces):\n",
    "        os.mkdir(d)\n",
    "\n",
    "    print(f\"Writing .jpg of audio's STFTs in {path_write_faces}\")\n",
    "    with torch.no_grad() :\n",
    "        mod, device = get_model()\n",
    "        for ix, video_name in tqdm(enumerate(video_names)):\n",
    "            \n",
    "            output_path = f\"{path_write_faces}{video_name.split('.')[0]}/\"\n",
    "            invid = f'{path_curr_vid}{video_name}'\n",
    "            dl = get_dataloader(invid)\n",
    "            try:\n",
    "                write_face_samples(mod, dl, output_path, device)\n",
    "            except:\n",
    "                print(f\"Couldnt yolo {invid}\")\n",
    "            \n",
    "            try:\n",
    "                write_stfts(video_name, ix == 0, path_curr_vid, output_path)\n",
    "            except:\n",
    "                print(f\"Couldnt stft {invid}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(\"dfdc_train_part_6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
